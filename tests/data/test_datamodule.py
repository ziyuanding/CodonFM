# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
import pytest
import torch
import lightning as L
from torch.utils.data import Dataset
from src.data.datamodule import CodonFMDataModule
import math

class SimpleIndexDataset(Dataset):
    """A simple dataset that returns its indices for easy testing."""
    def __init__(self, size=100, split='train'):
        self.size = size
        self.split = split
        self.data = list(range(size))
    
    def __len__(self):
        return self.size
    
    def __getitem__(self, idx):
        return {'index': self.data[idx], 'split': self.split}

class IndexTrackingDataset:
    """Dataset class that provides different splits while tracking indices.

    The ``train_size`` can be configured so that we can easily test scenarios where
    the dataset size **is** or **is not** a multiple of the training batch size.
    """
    def __init__(self, seed=None, train_size: int = 100):
        self.train_size = train_size
        self.val_size = 32
        self.test_size = 16
        
    def get_train(self, process_item=None):
        return SimpleIndexDataset(self.train_size, 'train')
    
    def get_validation(self, process_item=None):
        return SimpleIndexDataset(self.val_size, 'val')
    
    def get_test(self, process_item=None):
        return SimpleIndexDataset(self.test_size, 'test')

def collate_fn(batch):
    """Simple collate function that preserves indices."""
    return {
        'indices': torch.tensor([item['index'] for item in batch]),
        'split': [item['split'] for item in batch]
    }

@pytest.fixture(params=[100, 102], ids=["multiple_of_batch", "not_multiple_of_batch"])
def datamodule(request):
    """Create a ``CodonFMDataModule`` with a configurable training dataset size.

    Two configurations are tested:
        1. ``train_size`` **is** a multiple of ``train_batch_size`` (100 vs 4).
        2. ``train_size`` **is not** a multiple of ``train_batch_size`` (102 vs 4).
    """

    train_size = request.param
    train_batch_size = 4

    # ``train_iters`` chosen such that total samples ~= 1.5 epochs
    batches_per_epoch = math.ceil(train_size / train_batch_size)
    train_iters = math.ceil(batches_per_epoch * 1.5)

    def dataset_fn(seed=None):
        return IndexTrackingDataset(seed, train_size=train_size)

    return CodonFMDataModule(
        dataset=dataset_fn,
        num_workers=0,
        seed=42,
        world_size=1,
        train_iters=train_iters,
        collate_fn=collate_fn,
        train_batch_size=train_batch_size,
        val_batch_size=8,
        shuffle=True
    )

def test_train_dataloader_indices(datamodule):
    """Test that training dataloader properly handles indices with StatefulDataset."""
    datamodule.setup('fit')
    train_loader = datamodule.train_dataloader()
    
    # Get one full pass through the dataset
    all_indices = []
    for batch in train_loader:
        assert batch['split'][0] == 'train'  # Verify we're getting training data
        indices = batch['indices'].tolist()
        all_indices.extend(indices)
        # All batches should be full size since drop_last=True
        assert len(indices) == datamodule.train_batch_size
    
    # We should get complete batches up to the dataset size
    expected_total_samples = (datamodule.dataset.train_size // datamodule.train_batch_size) * datamodule.train_batch_size
    assert len(all_indices) == expected_total_samples
    
    # Verify that indices are within bounds of original dataset
    assert all(0 <= idx < datamodule.dataset.train_size for idx in all_indices)
    
    # Since shuffle is True, verify that we're getting shuffled indices
    assert sorted(all_indices) != all_indices  # not in order
    assert len(set(all_indices)) == len(all_indices)  # no repeats in first epoch

def test_train_dataloader_multiple_epochs(datamodule):
    """Test that StatefulDataset provides correct samples across epochs."""
    datamodule.setup('fit')
    
    # Get first epoch
    first_epoch_indices = []
    train_loader = datamodule.train_dataloader()
    for batch in train_loader:
        first_epoch_indices.extend(batch['indices'].tolist())
    
    # Create new datamodule with consumed_samples = dataset_size
    # This should give us samples from the second epoch
    new_datamodule = CodonFMDataModule(
        dataset=lambda seed: IndexTrackingDataset(seed, train_size=datamodule.dataset.train_size),
        seed=42,
        world_size=1,
        train_iters=datamodule.train_iters,
        collate_fn=collate_fn,
        train_batch_size=4,
        shuffle=True
    )
    new_datamodule.init_consumed_samples = datamodule.dataset.train_size 
    new_datamodule.setup('fit')
    
    # Get second epoch
    second_epoch_indices = []
    train_loader = new_datamodule.train_dataloader()
    for batch in train_loader:
        second_epoch_indices.extend(batch['indices'].tolist())
    

    assert len(first_epoch_indices) == len(second_epoch_indices)
    assert len(set(first_epoch_indices)) == len(first_epoch_indices)
    remaining_samples = (
        new_datamodule.train_iters * new_datamodule.global_batch_size
        - new_datamodule.init_consumed_samples
    )
    if remaining_samples >= datamodule.dataset.train_size:
        # We managed to consume (at least) one *full* epoch â†’ expect all unique.
        assert len(set(second_epoch_indices)) == len(second_epoch_indices)
    
    # 3. For datasets that are an exact multiple of the batch size we should see
    #    *all* indices in the epoch.  Otherwise we only get full batches up to
    #    ``floor(train_size / batch_size) * batch_size``.
    if datamodule.dataset.train_size % datamodule.train_batch_size == 0:
        assert sorted(first_epoch_indices) == list(range(datamodule.dataset.train_size))

        if remaining_samples >= datamodule.dataset.train_size:
            assert sorted(second_epoch_indices) == list(range(datamodule.dataset.train_size))
    
    # 4. Ensure the two epochs have different shuffle orders
    assert first_epoch_indices != second_epoch_indices


def test_eval_predict_and_test_dataloaders_identical_when_eval_mode():
    from torch.utils.data import Dataset
    from src.data.datamodule import CodonFMDataModule

    class TinyDataset(Dataset):
        def __init__(self, size=12, split='train'):
            self.size = size
            self.split = split
            self.data = list(range(size))
        def __len__(self):
            return self.size
        def __getitem__(self, idx):
            import torch
            return {'x': torch.tensor(self.data[idx]), 'split': self.split}

    class TinyProvider:
        def __init__(self, train_size=12, val_size=5, test_size=7):
            self.train_size = train_size
            self.val_size = val_size
            self.test_size = test_size
        def get_train(self, process_item=None):
            return TinyDataset(self.train_size, 'train')
        def get_validation(self, process_item=None):
            return TinyDataset(self.val_size, 'val')
        def get_test(self, process_item=None):
            return TinyDataset(self.test_size, 'test')

    def _collate_keep(batch):
        return batch

    dm = CodonFMDataModule(
        dataset=lambda seed: TinyProvider(),
        seed=42,
        world_size=1,
        train_iters=None,
        collate_fn=_collate_keep,
        train_batch_size=4,
        val_batch_size=2,
        shuffle=True,
        num_workers=0,
        is_evaluation=True,
    )

    dm.setup('predict')
    pred_dl = dm.predict_dataloader()
    test_dl = dm.test_dataloader()

    pred_batches = list(pred_dl)
    test_batches = list(test_dl)

    assert len(pred_batches) == len(test_batches)
    assert pred_batches[0][0]['split'] == 'test'
    assert test_batches[0][0]['split'] == 'test'


def test_train_uses_stateful_dataset_and_disables_shuffle(monkeypatch):
    from src.data.datamodule import CodonFMDataModule
    from src.data.stateful_dataset import StatefulDataset

    class TinyProvider:
        def __init__(self, train_size=10):
            self.train_size = train_size
        def get_train(self, process_item=None):
            from torch.utils.data import TensorDataset
            import torch
            return TensorDataset(torch.arange(self.train_size))
        def get_validation(self, process_item=None):
            from torch.utils.data import TensorDataset
            import torch
            return TensorDataset(torch.arange(5))
        def get_test(self, process_item=None):
            from torch.utils.data import TensorDataset
            import torch
            return TensorDataset(torch.arange(7))

    def _collate_keep(batch):
        return batch

    dm = CodonFMDataModule(
        dataset=lambda seed: TinyProvider(train_size=10),
        seed=42,
        world_size=1,
        train_iters=5,
        collate_fn=_collate_keep,
        train_batch_size=4,
        val_batch_size=2,
        shuffle=True,
        num_workers=0,
    )

    dm.setup('fit')
    train_dl = dm.train_dataloader()
    assert isinstance(train_dl.dataset, StatefulDataset)
    # DataLoader doesn't expose a 'shuffle' attribute; check sampler type instead.
    from torch.utils.data import sampler as _sampler
    assert isinstance(train_dl.sampler, _sampler.SequentialSampler)


def test_distributed_sampler_selected_when_dist_initialized(monkeypatch):
    from src.data import datamodule as dm_mod
    from src.data.datamodule import CodonFMDataModule
    import torch.distributed as dist

    class TinyProvider:
        def get_train(self, process_item=None):
            from torch.utils.data import TensorDataset
            import torch
            return TensorDataset(torch.arange(8))
        def get_validation(self, process_item=None):
            from torch.utils.data import TensorDataset
            import torch
            return TensorDataset(torch.arange(4))
        def get_test(self, process_item=None):
            from torch.utils.data import TensorDataset
            import torch
            return TensorDataset(torch.arange(4))

    def _collate_keep(batch):
        return batch

    monkeypatch.setattr(dist, 'is_initialized', lambda: True, raising=False)
    # Replace DistributedSampler with a lightweight stub to avoid initializing process group
    class _StubSampler:
        def __init__(self, dataset, shuffle=False, drop_last=False):
            self.dataset = dataset
            self.shuffle = shuffle
            self.drop_last = drop_last
    monkeypatch.setattr(dm_mod, 'DistributedSampler', _StubSampler, raising=True)

    dm = CodonFMDataModule(
        dataset=lambda seed: TinyProvider(),
        seed=42,
        world_size=2,
        train_iters=None,
        collate_fn=_collate_keep,
        train_batch_size=4,
        val_batch_size=2,
        shuffle=True,
        num_workers=0,
    )
    dm.setup('fit')
    train_dl = dm.train_dataloader()
    assert isinstance(train_dl.sampler, _StubSampler)

def test_consumed_samples_resumption(datamodule):
    """Test that resuming from consumed samples maintains proper indexing."""
    datamodule.setup('fit')
    
    # Get initial sequence of indices
    initial_indices = []
    train_loader = datamodule.train_dataloader()
    for i, batch in enumerate(train_loader):
        if i >= 2:  # Get first 2 batches
            break
        initial_indices.extend(batch['indices'].tolist())
    
    # Create new datamodule with consumed samples
    consumed_samples = len(initial_indices)
    new_datamodule = CodonFMDataModule(
        dataset=lambda seed: IndexTrackingDataset(seed, train_size=datamodule.dataset.train_size),
        seed=42,
        world_size=1,
        train_iters=datamodule.train_iters,
        collate_fn=collate_fn,
        train_batch_size=4,
        shuffle=True
    )
    new_datamodule.init_consumed_samples = consumed_samples
    new_datamodule.setup('fit')
    
    # Get next batch from new datamodule
    resumed_loader = new_datamodule.train_dataloader()
    resumed_batch = next(iter(resumed_loader))
    resumed_indices = resumed_batch['indices'].tolist()
    
    # Since we're using StatefulDataset with shuffling, we can't guarantee exact indices
    # but we can verify they're within bounds and the batch size is correct
    assert len(resumed_indices) == datamodule.train_batch_size  # Should always be full batch due to drop_last=True
    assert all(0 <= idx < datamodule.dataset.train_size for idx in resumed_indices)


def test_half_epoch_transition_resumption(datamodule):
    """Resume from half way through an epoch and verify correct continuation
    into the next epoch.
    """

    datamodule.setup('fit')

    # Reference indices for a complete first epoch
    reference_indices = []
    for batch in datamodule.train_dataloader():
        reference_indices.extend(batch['indices'].tolist())

    batches_per_epoch = len(reference_indices) // datamodule.train_batch_size
    # Resume after half the epoch (rounded down to full batch)
    resume_batches = batches_per_epoch // 2
    consumed_samples = resume_batches * datamodule.train_batch_size

    new_datamodule = CodonFMDataModule(
        dataset=lambda seed: IndexTrackingDataset(seed, train_size=datamodule.dataset.train_size),
        seed=42,
        world_size=1,
        train_iters=datamodule.train_iters,
        collate_fn=collate_fn,
        train_batch_size=datamodule.train_batch_size,
        shuffle=True
    )

    state_dict = {
        'consumed_samples': consumed_samples,
        'global_step': resume_batches
    }
    new_datamodule.load_state_dict(state_dict)
    new_datamodule.setup('fit')

    # How many batches are left in the current epoch?
    samples_remaining_in_epoch1 = len(reference_indices) - consumed_samples

    # Collect the remaining batches of epoch 1 *plus* two batches of epoch 2
    batches_to_collect = (samples_remaining_in_epoch1 // datamodule.train_batch_size) + 2

    resumed_indices = []
    for i, batch in enumerate(new_datamodule.train_dataloader()):
        if i >= batches_to_collect:
            break
        resumed_indices.extend(batch['indices'].tolist())

    # Split into tail of epoch 1 and head of epoch 2
    epoch1_end = resumed_indices[:samples_remaining_in_epoch1]
    epoch2_start = resumed_indices[samples_remaining_in_epoch1:]

    # ---- Assertions for end of epoch 1 ----
    assert len(epoch1_end) == samples_remaining_in_epoch1
    assert len(set(epoch1_end)) == len(epoch1_end)
    position_in_epoch = consumed_samples % datamodule.dataset.train_size
    expected_epoch1_end = reference_indices[position_in_epoch:position_in_epoch + samples_remaining_in_epoch1]
    assert epoch1_end == expected_epoch1_end

    # ---- Assertions for start of epoch 2 ----
    assert len(epoch2_start) == 2 * datamodule.train_batch_size
    assert len(set(epoch2_start)) == len(epoch2_start)
    epoch2_corresponding = reference_indices[:len(epoch2_start)]
    assert epoch2_start != epoch2_corresponding


def test_two_epoch_dataloader_calls(datamodule):
    """Ensure that successive calls to ``train_dataloader`` respect the number
    of already-consumed samples via ``trainer.global_step`` so that the second
    call begins at the start of the *next* epoch (with its own shuffle).
    """

    datamodule.setup('fit')
    # Attach a dummy trainer to drive global-step increment.
    DummyTrainer = type("DummyTrainer", (), {})
    datamodule.trainer = DummyTrainer()
    datamodule.trainer.global_step = 0

    # --------------------------- First epoch ---------------------------
    epoch1_indices = []
    for batch in datamodule.train_dataloader():
        epoch1_indices.extend(batch['indices'].tolist())

    batches_per_epoch = len(epoch1_indices) // datamodule.train_batch_size

    # Update global_step to reflect consumed batches (what lightning does).
    datamodule.trainer.global_step += batches_per_epoch

    # --------------------------- Second epoch --------------------------
    epoch2_indices = []
    for batch in datamodule.train_dataloader():
        epoch2_indices.extend(batch['indices'].tolist())

    expected_samples_per_epoch = (
        datamodule.dataset.train_size // datamodule.train_batch_size
    ) * datamodule.train_batch_size
    assert len(epoch1_indices) == expected_samples_per_epoch
    assert len(epoch2_indices) == expected_samples_per_epoch
    # Shuffle order between the two epochs should differ.
    assert epoch1_indices != epoch2_indices