{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Mutation Datasets Preprocessing\n",
    "\n",
    "This notebook preprocesses four mutation variant datasets for downstream analysis. For each dataset, we extract coding sequence (CDS) context from reference genome annotations, annotate variants with transcript information, codon changes, and amino acid translations, and save the processed data in a standardized format.\n",
    "\n",
    "## Required Pre-processing Steps\n",
    "\n",
    "Before generation the mutation sequences for zero-shot benchmarks, ensure that the following files are downloaded/processed and saved at `/data/ncbi`\n",
    "\n",
    "#### 1. Open-source Data Download\n",
    "\n",
    "\n",
    "##### Reference Files\n",
    "| File | Origin |\n",
    "|----------------|-------- |\n",
    "| `hg19.fa` | [Download](https://hgdownload.soe.ucsc.edu/goldenPath/hg19/bigZips/hg19.fa.gz) |\n",
    "| `hg38.fa` | [Download](https://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz) |\n",
    "\n",
    "##### Annotation Files\n",
    "\n",
    "| Annotation File | Origin | Table |\n",
    "|----------------|--------|-------|\n",
    "| `gencode.v47lift37.basic.annotation.gtf` | [GENCODE Release 47lift37](https://www.gencodegenes.org/human/release_47lift37.html) | - |\n",
    "| `ucsc_gencodev32_hg38.tsv` | [UCSC Table Browser](https://genome.ucsc.edu/cgi-bin/hgTables) | `wgEncodeGencodeCompV32` |\n",
    "| `ucsc_refseq_hg38.tsv` | [UCSC Table Browser](https://genome.ucsc.edu/cgi-bin/hgTables) | `ncbiRefSeq` |\n",
    "| `ucsc_refseq_hist_hg38.tsv` | [UCSC Table Browser](https://genome.ucsc.edu/cgi-bin/hgTables) | `ncbiRefSeqHistorical` |\n",
    "\n",
    "##### Variant Files\n",
    "\n",
    "| Variant File | Source | URL | Notes |\n",
    "|--------------|--------|-----|-------|\n",
    "| `asd_discov` | [Zhou et al. 2022](https://www.nature.com/articles/s41588-022-01148-2) | [Download](https://static-content.springer.com/esm/art%3A10.1038%2Fs41588-022-01148-2/MediaObjects/41588_2022_1148_MOESM5_ESM.xlsx) | Supplementary Table 5 |\n",
    "| `asd_rep` | [Zhou et al. 2022](https://www.nature.com/articles/s41588-022-01148-2) | [Download](https://static-content.springer.com/esm/art%3A10.1038%2Fs41588-022-01148-2/MediaObjects/41588_2022_1148_MOESM6_ESM.xlsx) | Supplementary Table 6 |\n",
    "| `ddd_other` | [Zhou et al. 2022](https://www.nature.com/articles/s41588-022-01148-2) | [Download](https://static-content.springer.com/esm/art%3A10.1038%2Fs41588-022-01148-2/MediaObjects/41588_2022_1148_MOESM7_ESM.xlsx) | Supplementary Table 7 |\n",
    "| `AlphaMissense ClinVar` | [Cheng et al. 2023](https://www.science.org/doi/10.1126/science.adg7492) | [Download](https://www.science.org/doi/suppl/10.1126/science.adg7492/suppl_file/science.adg7492_data_s1_to_s9.zip) | Data S5 |\n",
    "| `AlphaMissense CancerHotspot` | [Cheng et al. 2023](https://www.science.org/doi/10.1126/science.adg7492) | [Download](https://www.science.org/doi/suppl/10.1126/science.adg7492/suppl_file/science.adg7492_data_s1_to_s9.zip) | Data S6 |\n",
    "\n",
    "##### ClinVar Synonymous Matching Features\n",
    "\n",
    "| File | Source | URL |\n",
    "|------|--------|-----|\n",
    "| `hg38.phyloP447way.bw` | UCSC Genome Browser | [Download](https://hgdownload.soe.ucsc.edu/goldenPath/hg38/phyloP447way/hg38.phyloP447way.bw) |\n",
    "| `ucsc_pliByGene_hg38.tsv` | UCSC Genome Browser ‚Üí Table Browser | [Download](https://genome.ucsc.edu/cgi-bin/hgTables) (table: `pliByGene`) |\n",
    "| `variant_summary.txt.gz` | NCBI ClinVar (FTP) | [Download](https://ftp.ncbi.nlm.nih.gov/pub/clinvar/tab_delimited/variant_summary.txt.gz) |\n",
    "\n",
    "#### 2. Data Scripts\n",
    "\n",
    "Before running this notebook, ensure the following preprocessing scripts have been executed:\n",
    "\n",
    "| File | Purpose | How to Generate |\n",
    "|------|---------|-----------------|  \n",
    "| `codon_counts_nopathogen.json` | Codon counts by taxonomic group (used for codon frequency features) | Run `python data_scripts/check_codon_frequency.py` after completing NCBI preprocessing in `data_scripts/data_curation/`. Place or symlink the produced file at `/data/ncbi/codon_counts_nopathogen.json`. |\n",
    "| `gencode.v47lift37.basic.annotation.processed.tsv` | Processed GTF annotation with CDS coordinates | Run `python data_scripts/process_gtf.py` on the downloaded GENCODE GTF file `gencode.v47lift37.basic.annotation.gtf`. |\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "| Section | Dataset | Description | Required Data Files |\n",
    "|---------|---------|-------------|---------------------|\n",
    "| **1** | [DDD / ASD Dataset](#1-ddd-asd-dataset) | Developmental disorder and autism spectrum disorder variants | `ddd_asd_zhouetal/asd_discov.csv`<br>`ddd_asd_zhouetal/asd_rep.csv`<br>`ddd_asd_zhouetal/ddd_other.csv`<br>`gencode.v47lift37.basic.annotation.processed.tsv`<br>`alphamissense_data/AlphaMissense_hg19.tsv.gz`<br>`reference/hg19/hg19.fa` |\n",
    "| **2** | [ClinVar AlphaMissense](#2-clinvar-alphamissense-dataset) | ClinVar missense variants with AlphaMissense pathogenicity predictions | `alphamissense_data/alphamissense_clinvar.csv`<br>`ucsc_gencodev32_hg38.tsv`<br>`hg38/hg38.fa` |\n",
    "| **3** | [Cancer Hotspot](#3-cancer-hotspot) | Cancer hotspot mutations with AlphaMissense scores | `alphamissense_data/alphamissense_cancer_hotspot.csv`<br>`ucsc_gencodev32_hg38.tsv`<br>`reference/hg38/hg38.fa` |\n",
    "| **4** | [ClinVar Synonymous](#4-clinvar-synonymous) | Extract synonymous variants from ClinVar (benign and pathogenic labels) with optional additional features | `clinvar_syn/variant_summary.txt.gz`<br>`clinvar_syn/ucsc_refseq_hg38.tsv`<br>`clinvar_syn/ucsc_refseq_hist_hg38.tsv`<br>`reference/hg38/hg38.fa` |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Imports and Paths setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install PyBigWig\n",
    "#!pip install  pyBigWig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import pyBigWig\n",
    "import pyfaidx\n",
    "import seaborn as sns\n",
    "from Bio.Seq import Seq\n",
    "from matplotlib.ticker import LogLocator\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Before setting the `DATA_DIR` path, ensure the following directory structure (containing the files from the required pre-processing steo) is in place:\n",
    "\n",
    "```\n",
    "üìÅ DATA_DIR/\n",
    "‚îú‚îÄ‚îÄ üìÅ alphamissense_data/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ AlphaMissense_hg19.tsv.gz\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ alphamissense_cancer_hotspot.csv\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ alphamissense_clinvar.csv\n",
    "‚îú‚îÄ‚îÄ üìÅ ddd_asd_zhouetal/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ asd_discov.csv\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ asd_rep.csv\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ddd_other.csv\n",
    "‚îú‚îÄ‚îÄ üìÅ clinvar_syn/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ variant_summary.txt.gz\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ ucsc_refseq_hg38.tsv\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ ucsc_refseq_hist_hg38.tsv\n",
    "‚îú‚îÄ‚îÄ üìÅ reference/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ hg19/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hg19.fa\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ hg19.fa.fai\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ hg38/\n",
    "‚îÇ       ‚îú‚îÄ‚îÄ hg38.fa\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ hg38.fa.fai\n",
    "‚îú‚îÄ‚îÄ üìÑ codon_counts_nopathogen.json\n",
    "‚îú‚îÄ‚îÄ üìÑ gencode.v47lift37.basic.annotation.processed.tsv\n",
    "‚îú‚îÄ‚îÄ üìÑ ucsc_gencodev32_hg38.tsv\n",
    "‚îú‚îÄ‚îÄ üìÑ ucsc_pliByGene_hg38.tsv\n",
    "‚îî‚îÄ‚îÄ üìÑ hg38.phyloP447way.bw\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/data/ncbi/\" # set this to the path of your data directory\n",
    "\n",
    "OUTPUT_DIR = \"/data/validation/processed/\"  # output directory where all processed datasets will be saved\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# 0. Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dna_code = { \"ATA\": \"I\", \"ATC\": \"I\", \"ATT\": \"I\", \"ATG\": \"M\", \"ACA\": \"T\", \"ACC\": \"T\", \"ACG\": \"T\", \"ACT\": \"T\", \"AAC\": \"N\", \"AAT\": \"N\", \"AAA\": \"K\", \"AAG\": \"K\", \"AGC\": \"S\", \"AGT\": \"S\", \"AGA\": \"R\", \"AGG\": \"R\", \"CTA\": \"L\", \"CTC\": \"L\", \"CTG\": \"L\", \"CTT\": \"L\", \"CCA\": \"P\", \"CCC\": \"P\", \"CCG\": \"P\", \"CCT\": \"P\", \"CAC\": \"H\", \"CAT\": \"H\", \"CAA\": \"Q\", \"CAG\": \"Q\", \"CGA\": \"R\", \"CGC\": \"R\", \"CGG\": \"R\", \"CGT\": \"R\", \"GTA\": \"V\", \"GTC\": \"V\", \"GTG\": \"V\", \"GTT\": \"V\", \"GCA\": \"A\", \"GCC\": \"A\", \"GCG\": \"A\", \"GCT\": \"A\", \"GAC\": \"D\", \"GAT\": \"D\", \"GAA\": \"E\", \"GAG\": \"E\", \"GGA\": \"G\", \"GGC\": \"G\", \"GGG\": \"G\", \"GGT\": \"G\", \"TCA\": \"S\", \"TCC\": \"S\", \"TCG\": \"S\", \"TCT\": \"S\", \"TTC\": \"F\", \"TTT\": \"F\", \"TTA\": \"L\", \"TTG\": \"L\", \"TAC\": \"Y\", \"TAT\": \"Y\", \"TAA\": \"*\", \"TAG\": \"*\", \"TGC\": \"C\", \"TGT\": \"C\", \"TGA\": \"*\", \"TGG\": \"W\",\n",
    "}\n",
    "\n",
    "\n",
    "def translate(seq):\n",
    "    \"\"\"\n",
    "    Translate an RNA sequence into a protein sequence.\n",
    "    \"\"\"\n",
    "    protein = \"\"\n",
    "    # Process the RNA sequence three nucleotides (codon) at a time.\n",
    "    for i in range(0, len(seq) - 2, 3):\n",
    "        codon = seq[i : i + 3]\n",
    "        # Look up the codon in the genetic code dictionary.\n",
    "        amino_acid = dna_code.get(codon, \"?\")\n",
    "        protein += amino_acid\n",
    "    return protein\n",
    "\n",
    "def reverse_complement_dna(seq):\n",
    "    \"\"\"\n",
    "    Return the reverse complement of a DNA sequence.\n",
    "    \n",
    "    Parameters:\n",
    "        seq (str): A DNA sequence with uppercase letters only (e.g., \"ATCG\").\n",
    "    \n",
    "    Returns:\n",
    "        str: The reverse complement DNA sequence.\n",
    "        \n",
    "    Raises:\n",
    "        KeyError: If the sequence contains lowercase letters or invalid characters.\n",
    "    \"\"\"\n",
    "    complement = {'A': 'T', 'T': 'A', 'G': 'C', 'C': 'G', 'N': 'N'}\n",
    "    return ''.join(complement[base] for base in seq[::-1])\n",
    "\n",
    "\n",
    "def process_gtf(gtf_path, fasta_path):\n",
    "    \"\"\"\n",
    "    Build coding sequences (CDS) for transcripts from a a gtf annotation file\n",
    "    and a reference FASTA file.\n",
    "    \"\"\"\n",
    "    gtf = pd.read_csv(gtf_path, sep='\\t')\n",
    "    if '#name' in gtf.columns:\n",
    "        gtf = gtf.rename({'#name':'name'}, axis=1)\n",
    "    gtf =gtf.loc[gtf['cdsStart']!=gtf['cdsEnd']].reset_index(drop=True).copy() # keep transcripts with a non‚Äëempty coding region\n",
    "\n",
    "    gtf['exonStarts_arr'] = gtf['exonStarts'].map(lambda x:ast.literal_eval(x))\n",
    "    gtf['exonEnds_arr'] = gtf['exonEnds'].map(lambda x:ast.literal_eval(x))\n",
    "    fasta = {}\n",
    "    with pyfaidx.Fasta(fasta_path) as f:\n",
    "        # Get available chromosomes in the FASTA file\n",
    "        available_chroms = set(f.keys())\n",
    "        # Filter GTF to only include chromosomes present in FASTA\n",
    "        gtf_chroms = set(gtf['chrom'].unique())\n",
    "        missing_chroms = gtf_chroms - available_chroms\n",
    "        if missing_chroms:\n",
    "            print(f\"Warning: {len(missing_chroms)} chromosome(s) in GTF not found in FASTA, filtering them out: {sorted(missing_chroms)}\")\n",
    "            gtf = gtf[gtf['chrom'].isin(available_chroms)].reset_index(drop=True).copy()\n",
    "        # Load chromosome sequences\n",
    "        for chrom in gtf['chrom'].unique():\n",
    "            fasta[chrom] = f[chrom][:].seq.upper() #  load entire chrom sequence to uppercase and store in fasta[chrom] for fast slicing.\n",
    "    cds_starts = []\n",
    "    cds_ends = []\n",
    "    lengths = []\n",
    "    seqs = []\n",
    "    for i in tqdm(range(gtf.shape[0]), desc=\"Processing transcripts\", total=gtf.shape[0]): # loop through each transcript in the gtf file\n",
    "        t = gtf.iloc[i]\n",
    "        chrom = t['chrom']\n",
    "        cds_s = []\n",
    "        cds_e = []\n",
    "        cs,ce = t[['cdsStart','cdsEnd']] # These are 0-based coordinates\n",
    "        length = 0\n",
    "        curr_seq = []\n",
    "        for a,b in zip(t['exonStarts_arr'],t['exonEnds_arr']): # combine all exons regions for this transcript\n",
    "            v1 = max(a,cs) # clip exons to the coding region (ignore UTRs)\n",
    "            v2 = min(b,ce)\n",
    "            if v1 < v2: # record the sequence string if it overlaps with the coding region\n",
    "                cds_s.append(v1)\n",
    "                cds_e.append(v2)\n",
    "                length += v2-v1\n",
    "                curr_seq.append(fasta[chrom][v1:v2])\n",
    "        # save the cds starts, ends, and length for this transcript\n",
    "        cds_starts.append(tuple(cds_s)) \n",
    "        cds_ends.append( tuple(cds_e))\n",
    "        lengths.append(length)\n",
    "        # Get the joined CDS sequence in the forward direction (as it is build from the reference FASTA)\n",
    "        curr_seq = ''.join(curr_seq)\n",
    "        if t['strand'] == '-':\n",
    "            # reverse‚Äëcomplement to get the seequence in the gene direction (5'‚Üí3')\n",
    "            curr_seq = reverse_complement_dna(curr_seq)\n",
    "        seqs.append(curr_seq)\n",
    "    # Build the the processed transcript table with coding sequence information for each transcript\n",
    "    gtf['cds_starts'] = cds_starts\n",
    "    gtf['cds_ends'] = cds_ends\n",
    "    gtf['cds_length'] = lengths\n",
    "    gtf['cds'] = seqs # sequence is strand-aware (always gene 5'->3')\n",
    "\n",
    "    gtf_s = gtf[['name','chrom','strand','cdsStart','cdsEnd','cds_starts','cds_ends','cds_length','cds']].copy()\n",
    "    gtf_s['name'] = gtf_s['name'].str.split('.').str[0]\n",
    "    # Sort transcripts by chromosome, start, and end coordinates, they're in the forward direction and 0-based.\n",
    "    gtf_s = gtf_s.sort_values(by=['chrom','cdsStart','cdsEnd']).reset_index(drop=True).copy() \n",
    "    \n",
    "    return gtf_s, fasta\n",
    "\n",
    "\n",
    "def process_a_chrom(chrom_variants, chrom_refseq, return_alt_cds=False):\n",
    "    \"\"\"\n",
    "    Annotate a single nucleotide variant with transcript CDS context.\n",
    "        - Locate its position within the coding sequence (0-based coordinate),\n",
    "        - extract the ref codon and build the alt codon,\n",
    "        - translate codons to amino acids,\n",
    "        - optionally, build the full alternate CDS with the mutation.\n",
    "    \n",
    "    ## Notes: The output columns are: \n",
    "       - pos: 1-based (forward direction, genomic coordinates)\n",
    "       - ref: genomic ref allele (as in the reference genome, forward orientation)\n",
    "       - alt: genomic alt allele (as in the reference genome, forward orientation)\n",
    "       - cdsStart: 0-based genomic start of CDS, half-open interval; genomic axis.\n",
    "       - cdsEnd: 0-based genomic end of CDS, half-open interval; genomic axis.\n",
    "       - var_rel_dist_in_cds: 0-based index within the CDS in transcript 5'->3' orientation (strand-aware).\n",
    "       - ref_seq: full CDS string in transcript 5'->3' orientation\n",
    "       - ref_codon: codon from ref_seq at the variant position; strand-aware.\n",
    "       - alt_codon: codon after single-base change; strand-aware.\n",
    "       - ref_aa: ref amino acid at the variant position\n",
    "       - alt_aa: alt amino acid at the variant position\n",
    "       - alt_seq (if set): full alt CDS after the single-base change, transcript 5'->3' orientation; strand-aware.\n",
    "       - codon_position: 0-based codon index within CDS.\n",
    "\n",
    "    \"\"\"\n",
    "    # normalize alleles;\n",
    "    chrom_variants['ref'] = chrom_variants['ref'].str.upper()\n",
    "    chrom_variants['alt'] = chrom_variants['alt'].str.upper()\n",
    "\n",
    "\n",
    "    var_ids = chrom_variants['variant_id'].values\n",
    "    var_pos = chrom_variants['pos'].values - 1  # Convert to 0-based - mutations are always reported in 1-based coordinates\n",
    "    var_ref = chrom_variants['ref'].values # reference allele\n",
    "    var_alt = chrom_variants['alt'].values # alternate allele\n",
    "    chrom = chrom_refseq.iloc[0]['chrom']\n",
    "    \n",
    "    # CDS processed using `process_gtf`` function\n",
    "    cds_strands = chrom_refseq['strand'].values # strand of the coding sequence\n",
    "    cds_starts = chrom_refseq['cdsStart'].values  \n",
    "    cds_ends = chrom_refseq['cdsEnd'].values\n",
    "    cds_lengths = chrom_refseq['cds_length'].values\n",
    "    rec_cds_starts = chrom_refseq['cds_starts'].values  # List of exon starts within the CDS region for all transcripts.\n",
    "    rec_cds_ends = chrom_refseq['cds_ends'].values  # List of exon ends within the CDS region for all transcripts.\n",
    "    rec_cds = chrom_refseq['cds'].values  # CDS sequence - strand-aware (always gene 5'->3')\n",
    "    rec_names = chrom_refseq['name'].values\n",
    "\n",
    "    # Find transcripts (CDS regions)that overlap the variant position\n",
    "    # sorted variant positions to find, per transcript, \n",
    "    # var_pos[s1[j]:ss2[j]] includes all variants with pos in [cdsStart[j], cdsEnd[j]) for transcript j. \n",
    "    s1 = np.searchsorted(var_pos, cds_starts, side='left')\n",
    "    s2 = np.searchsorted(var_pos, cds_ends, side='right')\n",
    "    results = []\n",
    "\n",
    "    # Loop through each transcript j (CDS region) and get information for all overlapping variants.\n",
    "    for j, (ss1, ss2) in enumerate(zip(s1, s2)):\n",
    "        curr_starts = rec_cds_starts[j] # CDS starts boundaries for transcript j.\n",
    "        curr_ends = rec_cds_ends[j] # CDS ends boundaries for transcript j.\n",
    "        # Sanity checks on CDS sequence for this transcript\n",
    "        assert cds_lengths[j] == len(rec_cds[j]), f\"CDS length mismatch for {rec_names[j]}\"\n",
    "        assert cds_lengths[j] % 3 == 0, f\"CDS length not multiple of 3 for {rec_names[j]}\"\n",
    "        if ss1 < ss2:\n",
    "            for i in range(ss1, ss2): # loop through all variants in the CDS region for transcript j.\n",
    "                pos = var_pos[i]\n",
    "                curr_ref = var_ref[i]\n",
    "                curr_alt = var_alt[i]\n",
    "                # Calculate offset in CDS sequence - translate a genomic coordinate into a CDS relativeindex.\n",
    "                offset = 0\n",
    "                bound = False\n",
    "                for a,b in zip(curr_starts, curr_ends):\n",
    "                    if pos >= b: # if the variant is after the end of the exon\n",
    "                        offset += b-a  # Add length of complete exon\n",
    "                    elif a <= pos < b:\n",
    "                        offset += pos-a \n",
    "                        bound = True  # the variant is within this exon\n",
    "                        break\n",
    "                if bound:\n",
    "                    if cds_strands[j] == '-':\n",
    "                        # Handle reverse strand\n",
    "                        offset = cds_lengths[j]-1-offset  # Convert to reverse strand position\n",
    "                        ref_codon = rec_cds[j][offset//3*3:offset//3*3+3]\n",
    "                        # Check if the reference base in ref codon is the reverse complement of the variant reference allele from the reference FASTA.\n",
    "                        assert rec_cds[j][offset] == reverse_complement_dna(curr_ref), f'-, {ref_codon} {var_ids[i]}'\n",
    "                        # Build the alternate codon by replacing the reference base with the alternate allele.\n",
    "                        alt_codon = ref_codon[:offset%3] + reverse_complement_dna(curr_alt) + ref_codon[offset%3+1:]\n",
    "                        results.append([chrom, pos, f'{chrom}_{pos+1}_{curr_ref}_{curr_alt}', curr_ref, curr_alt,\n",
    "                                     rec_names[j], cds_starts[j], cds_ends[j], cds_strands[j], offset, rec_cds[j],\n",
    "                                     ref_codon, alt_codon, translate(ref_codon), translate(alt_codon)])\n",
    "                        \n",
    "                        if return_alt_cds:\n",
    "                            alt_cds = rec_cds[j][:offset] + reverse_complement_dna(curr_alt) + rec_cds[j][offset+1:]\n",
    "                            results[-1].append(alt_cds)\n",
    "                    else:\n",
    "                        # Handle forward strand\n",
    "                        ref_codon = rec_cds[j][offset//3*3:offset//3*3+3]\n",
    "                        assert rec_cds[j][offset] == curr_ref, f'+, {ref_codon} {var_ids[i]}'\n",
    "                        alt_codon = ref_codon[:offset%3] + curr_alt + ref_codon[offset%3+1:]\n",
    "                        \n",
    "                        results.append([chrom, pos, f'{chrom}_{pos+1}_{curr_ref}_{curr_alt}', curr_ref, curr_alt,\n",
    "                                     rec_names[j], cds_starts[j], cds_ends[j], cds_strands[j], offset, rec_cds[j],\n",
    "                                     ref_codon, alt_codon, translate(ref_codon), translate(alt_codon)])\n",
    "                        \n",
    "                        if return_alt_cds:\n",
    "                            alt_cds = rec_cds[j][:offset] + curr_alt + rec_cds[j][offset+1:]\n",
    "                            results[-1].append(alt_cds)\n",
    "\n",
    "    columns = ['chrom','pos','variant_id','ref','alt','tx_name', 'cdsStart','cdsEnd','tx_strand',\n",
    "              'var_rel_dist_in_cds', 'ref_seq','ref_codon','alt_codon','ref_aa','alt_aa']\n",
    "    if return_alt_cds:\n",
    "        columns.append('alt_seq')\n",
    "    \n",
    "    if results:\n",
    "        results = pd.DataFrame(results)\n",
    "        results.columns = columns\n",
    "        results['pos'] += 1  # Convert back to 1-based\n",
    "        results['codon_position'] = results['var_rel_dist_in_cds']//3\n",
    "    else:\n",
    "        # Create empty DataFrame with correct columns\n",
    "        results = pd.DataFrame(columns=columns)\n",
    "        results['codon_position'] = pd.Series(dtype='int64')\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def plot_transcript_distribution(variants):\n",
    "    # Count transcripts per variant, then count how many variants fall in each bin\n",
    "    counts_pl = pl.from_pandas(variants).group_by('variant_id').count().rename({'count': 'n_transcripts'})\n",
    "    hist_pl = (counts_pl\n",
    "            .group_by('n_transcripts')\n",
    "            .count()\n",
    "            .rename({'count': 'n_variants'})\n",
    "            .sort('n_transcripts'))\n",
    "    df = hist_pl.to_pandas().astype({'n_transcripts':'int64','n_variants':'int64'})\n",
    "    plt.figure(figsize=(20, 4))\n",
    "    ax = sns.barplot(data=df, x='n_transcripts', y='n_variants', color='#4c5a88')\n",
    "    ax.set_xlabel('Number of transcripts associated with a single variant')\n",
    "    ax.set_ylabel('Number of unique variants')\n",
    "    ax.set_title('Distribution of transcripts per variant')  \n",
    "    ax.set_yscale('log')  \n",
    "    ax.yaxis.set_major_locator(LogLocator(base=10)) # 10^k ticks                        # log-scale Y\n",
    "    for p in ax.patches:\n",
    "        h = p.get_height()\n",
    "        if h > 0:\n",
    "            ax.annotate(f'{int(h):,}', (p.get_x()+p.get_width()/2, h),\n",
    "                        ha='center', va='bottom', fontsize=9, xytext=(0, 3),\n",
    "                        textcoords='offset points')\n",
    "    sns.despine()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def check_mutation_positions(df: pd.DataFrame, adjusted_context_length: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Check if the mutation positions are within the bounds of the coding sequence.\n",
    "    adjusted_context_length = max_length - 2 (for [CLS] and [SEP])\n",
    "    \"\"\"\n",
    "    def _check(row):\n",
    "        ref_seq = row[\"ref_seq\"]\n",
    "        codon_pos = int(row[\"codon_position\"])\n",
    "        total_codons = (len(ref_seq) // 3) if isinstance(ref_seq, str) else 0\n",
    "        out = {\n",
    "            \"id\": row.get(\"id\", None),\n",
    "            \"variant_id\": row.get(\"variant_id\", None),\n",
    "            \"total_codons\": total_codons,\n",
    "            \"codon_position\": codon_pos,\n",
    "            \"in_bounds\": codon_pos < total_codons,\n",
    "            \"needs_centering\": total_codons > adjusted_context_length,\n",
    "            \"out_of_bounds\": codon_pos >= adjusted_context_length,\n",
    "        }\n",
    "        return pd.Series(out)\n",
    "    return df.apply(_check, axis=1)\n",
    "\n",
    "\n",
    "def get_reverse_complement(seq):\n",
    "    \"\"\"Get reverse complement of a sequence\"\"\"\n",
    "    complement = {'A': 'T', 'T': 'A', 'G': 'C', 'C': 'G', 'N': 'N'}\n",
    "    return ''.join(complement[base] for base in seq[::-1].upper())\n",
    "\n",
    "\n",
    "def extract_cds_sequence(row, fasta):\n",
    "    \"\"\"Extract CDS sequence for a transcript based on exon coordinates and CDS boundaries.\"\"\"\n",
    "    chrom = row['chrom']\n",
    "    strand = row['strand']\n",
    "    cds_start = row['cdsStart']\n",
    "    cds_end = row['cdsEnd']\n",
    "    \n",
    "    # Parse exon coordinates\n",
    "    exon_starts = [int(x) for x in row['exonStarts'].rstrip(',').split(',')]\n",
    "    exon_ends = [int(x) for x in row['exonEnds'].rstrip(',').split(',')]\n",
    "    \n",
    "    # Extract CDS sequence from exons\n",
    "    cds_sequence = \"\"\n",
    "    \n",
    "    for start, end in zip(exon_starts, exon_ends):\n",
    "        # Find overlap between exon and CDS\n",
    "        overlap_start = max(start, cds_start)\n",
    "        overlap_end = min(end, cds_end)\n",
    "        \n",
    "        if overlap_start < overlap_end:\n",
    "            # Extract sequence from this exon segment\n",
    "            seq = str(fasta[chrom][overlap_start:overlap_end]).upper()\n",
    "            cds_sequence += seq\n",
    "    \n",
    "    # Reverse complement if on negative strand\n",
    "    if strand == '-':\n",
    "        cds_sequence = get_reverse_complement(cds_sequence)\n",
    "    \n",
    "    return cds_sequence\n",
    "\n",
    "\n",
    "def process_dset(dset, refseq, remove_non_pli=False):\n",
    "    \"\"\"\n",
    "    Add additional features to the dataset including:\n",
    "    - Amino acid translations\n",
    "    - Codon frequency ratios\n",
    "    - Gene names and pLI scores\n",
    "    - PhyloP conservation scores\n",
    "    - CDS offset fractions\n",
    "    \"\"\"\n",
    "    # Add amino acid translations to the dataset\n",
    "    dset = dset.with_columns([\n",
    "        pl.col('ref_codon').map_elements(lambda x: str(Seq(x).translate()), return_dtype=pl.String).alias('ref_aa'),\n",
    "        pl.col('alt_codon').map_elements(lambda x: str(Seq(x).translate()), return_dtype=pl.String).alias('alt_aa')\n",
    "    ])\n",
    "\n",
    "    assert dset.filter(pl.col('ref_aa') != pl.col('alt_aa')).height == 0\n",
    "    dset = dset.filter(pl.col('ref_aa') != '*')\n",
    "\n",
    "    codon_freqs = json.load(open(f'{DATA_DIR}/codon_counts_nopathogen.json'))['Primates']\n",
    "\n",
    "    dset = dset.with_columns(pl.col('ref_codon').map_elements(lambda x: codon_freqs[x], return_dtype=pl.Float64).alias('ref_codon_freq'))\n",
    "    dset = dset.with_columns(pl.col('alt_codon').map_elements(lambda x: codon_freqs[x], return_dtype=pl.Float64).alias('alt_codon_freq'))\n",
    "    dset = dset.with_columns((pl.col('ref_codon_freq') / pl.col('alt_codon_freq')).log().alias('codon_freq_ratio'))\n",
    "\n",
    "    tx_to_name = {row['name']: row['name2'] for row in refseq.rows(named=True)}\n",
    "\n",
    "    if 'gene_name' not in dset.columns:\n",
    "        dset = dset.with_columns(pl.col('tx').map_elements(lambda x: tx_to_name[x], return_dtype=pl.String).alias('gene_name'))\n",
    "    pli = pl.read_csv(f'{DATA_DIR}/ucsc_pliByGene_hg38.tsv', separator='\\t')\n",
    "    gene_to_pli = {row['geneName']: row['_pli'] for row in pli.rows(named=True)}\n",
    "    dset = dset.with_columns(pl.col('gene_name').map_elements(lambda x: gene_to_pli.get(x, -1000), return_dtype=pl.Float64).alias('pli'))\n",
    "\n",
    "    if remove_non_pli:\n",
    "        dset = dset.filter(pl.col('pli') != -1000)\n",
    "    dset = dset.with_columns((pl.col('pli')*10).cast(pl.Int32).alias('pli_bin'))\n",
    "\n",
    "    bw = pyBigWig.open(f'{DATA_DIR}/hg38.phyloP447way.bw')\n",
    "    phylop = []\n",
    "    for row in tqdm(dset.rows(named=True)):\n",
    "        phylop.append(bw.values(row['chrom'], row['pos']-1, row['pos'])[0])\n",
    "    dset = dset.with_columns(pl.Series(values=phylop, name='phylop').fill_nan(-1000))\n",
    "    dset = dset.with_columns(pl.col('phylop').round().cast(pl.Int32).alias('phylop_bin'))\n",
    "\n",
    "    if 'cds_offset_frac' not in dset.columns:\n",
    "        dset = dset.with_columns(pl.col('ref_seq').str.len_chars().alias('cds_length'))\n",
    "        dset = dset.with_columns((pl.col('var_rel_dist_in_cds') / pl.col('cds_length')).alias('cds_offset_frac'))\n",
    "    dset = dset.with_columns((pl.col('cds_offset_frac')*10).cast(pl.Int32).alias('cds_offset_frac_bin'))\n",
    "\n",
    "    return dset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "# 1. DDD-ASD Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"ddd_asd_zhouetal\"\n",
    "# Load ASD variants\n",
    "data = pd.read_csv(f'{DATA_DIR}/{dataset_name}/asd_discov.csv')\n",
    "data1 = pd.read_csv(f'{DATA_DIR}/{dataset_name}/asd_rep.csv')\n",
    "data = pd.concat([data, data1])\n",
    "print(\"ASD variants:\")\n",
    "display(data.head(2))\n",
    "# Load DDD variants\n",
    "ddd_variants = pd.read_csv(f'{DATA_DIR}/{dataset_name}/ddd_other.csv')\n",
    "print(\"DDD variants:\")\n",
    "display(ddd_variants.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for non SNVs\n",
    "(ddd_variants['Ref'].str.len() != 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitler SNVs in ASD and DDD datasets\n",
    "data = data.loc[(data['Ref'].str.len() == 1) & (data['Alt'].str.len() == 1)].copy()\n",
    "asd_variants = data[data['Pheno'] == 'Affected']\n",
    "ct_variants = data[data['Pheno'] == 'Unaffected']\n",
    "\n",
    "ddd_variants = ddd_variants.loc[(ddd_variants['Ref'].str.len() == 1) & (ddd_variants['Alt'].str.len() == 1)].copy()\n",
    "\n",
    "print(\"ASD variants counts:\")\n",
    "print(data['Pheno'].value_counts())\n",
    "print(\"\\nDDD variants counts:\")\n",
    "print(ddd_variants['Pheno'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group all variants \n",
    "variants = []\n",
    "for classification, dset in zip(['asd', 'control', 'ddd'], [asd_variants, ct_variants, ddd_variants]):\n",
    "    temp = dset[['VarID', 'Chrom', 'Position', 'Ref', 'Alt', 'Pheno']].copy()\n",
    "    temp['classification'] = classification\n",
    "    variants.append(temp)\n",
    "variants = pd.concat(variants)\n",
    "\n",
    "# Rename columns\n",
    "variants = variants.rename(columns={'VarID': 'variant_id', 'Chrom': 'chrom', 'Position': 'pos', 'Ref': 'ref', 'Alt': 'alt'})\n",
    "variants['chrom'] = 'chr' + variants['chrom'].astype(str)\n",
    "variants = variants.sort_values(by=['chrom', 'pos']).reset_index(drop=True).copy()\n",
    "\n",
    "variants.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the reference sequence and CDS context annotation of the variants from the GTF and FASTA files\n",
    "assembly = 'hg19'\n",
    "all_results = []\n",
    "gtf_s, fasta = process_gtf(f'{DATA_DIR}/gencode.v47lift37.basic.annotation.processed.tsv', f'{DATA_DIR}/reference/{assembly}/hg19.fa')\n",
    "print(f\"Processed {gtf_s.shape[0]} GTF CDS sequences\")\n",
    "display(gtf_s[['name', 'chrom', 'strand', 'cdsStart', 'cdsEnd', 'cds_starts', 'cds_ends', 'cds_length']].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for transcripts with CDS length not multiple of 3\n",
    "gtf_s[gtf_s['cds_length'].astype(int) % 3 != 0][['name', 'chrom', 'strand', 'cdsStart', 'cdsEnd', 'cds_starts', 'cds_ends', 'cds_length']].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process variants per chromosome\n",
    "## Filter sequences with CDS length not multiple of 3\n",
    "gtf_s = gtf_s[gtf_s['cds_length'].astype(int) % 3 == 0]\n",
    "chroms = variants['chrom'].unique()\n",
    "for chrom in tqdm(chroms, desc=\"Processing chromosomes\", total=len(chroms)):\n",
    "    curr_variants = variants[variants['chrom']==chrom][['variant_id', 'chrom','pos','ref','alt']].drop_duplicates().copy()\n",
    "    chrom_gtf = gtf_s[gtf_s['chrom']==chrom]\n",
    "    chrom_results = process_a_chrom(curr_variants, chrom_gtf, return_alt_cds=True)\n",
    "    all_results.append(chrom_results)\n",
    "all_results = pd.concat(all_results).reset_index(drop=True)\n",
    "all_results['variant_id'] = all_results['variant_id'] + '_' + assembly\n",
    "all_results = all_results.merge(variants.drop('variant_id', axis=1), left_on=['chrom','pos','ref','alt'], \n",
    "                                right_on=['chrom','pos','ref','alt'])\n",
    "print(f\"\\n Processed {all_results.shape[0]} mutations with CDS context:\")\n",
    "cols_to_display = [\n",
    "    'chrom', 'pos', 'variant_id', 'ref', 'alt', 'tx_name', 'cdsStart',\n",
    "    'cdsEnd', 'tx_strand', 'var_rel_dist_in_cds', 'ref_codon', 'alt_codon', 'ref_aa', 'alt_aa', \n",
    "    'codon_position', 'Pheno','classification']\n",
    "display(all_results[cols_to_display].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure reference allele in fasta matches reference allele in variants\n",
    "for i in range(variants.shape[0]):\n",
    "    t = variants.iloc[i]\n",
    "    variant_id = t['variant_id']\n",
    "    chrom = t['chrom']\n",
    "    pos = t['pos']\n",
    "    ref = t['ref']\n",
    "    hg19_ref = fasta[chrom][pos-1]\n",
    "    if hg19_ref != ref:\n",
    "        print(f\"Mismatch at {chrom}:{pos}, {ref} != {hg19_ref}, {variant_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add variant type based on translated amino acid\n",
    "all_results['variant_type'] = 'synonymous'\n",
    "all_results.loc[(all_results['ref_aa'] != all_results['alt_aa']) & (all_results['alt_aa'] != '*'), 'variant_type'] = 'missense'\n",
    "all_results.loc[(all_results['ref_aa'] != all_results['alt_aa']) & (all_results['alt_aa'] == '*'), 'variant_type'] = 'ptv'\n",
    "all_results.loc[ (all_results['ref_aa'] == '*'), 'variant_type'] = 'in_stop'\n",
    "all_results[cols_to_display + ['variant_type']].head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results[['variant_type', 'classification']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = (all_results\n",
    " .groupby(['variant_id', 'tx_name', 'variant_type'])\n",
    " .agg({\n",
    "     col: 'first' if col not in ['classification'] else lambda x: ','.join(sorted(set(x)))\n",
    "     for col in all_results.columns\n",
    "     if col not in ['variant_id', 'tx_name', 'variant_type']\n",
    " })\n",
    " .reset_index()\n",
    ")\n",
    "\n",
    "# Find rows where classification contains both control and (asd or ddd)\n",
    "mask = final_results['classification'].str.contains('control') & (\n",
    "    final_results['classification'].str.contains('asd') | \n",
    "    final_results['classification'].str.contains('ddd')\n",
    ")\n",
    "final_results = final_results[~mask].reset_index(drop=True)\n",
    "final_results = final_results.drop_duplicates(['variant_id','ref_seq','alt_seq','ref_codon','alt_codon'])\n",
    "final_results.insert(0, 'id', np.arange(final_results.shape[0]))\n",
    "final_results[cols_to_display + ['variant_type']].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results['variant_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get both unique variants and total rows per group\n",
    "variant_counts = final_results.groupby(['classification', 'variant_type']).agg({\n",
    "    'variant_id': ['nunique', 'count']\n",
    "}).rename(columns={'nunique': 'unique_variants', 'count': 'total_rows'})\n",
    "print(variant_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "af_hg19 = pl.read_csv(f'{DATA_DIR}/alphamissense_data/AlphaMissense_hg19.tsv.gz', separator='\\t', skip_rows=3)\n",
    "af_hg19 = af_hg19.rename({'#CHROM': 'chrom', 'POS': 'pos', 'REF': 'ref', 'ALT': 'alt'})\n",
    "af_hg19 = af_hg19.with_columns(pl.concat_str([\n",
    "    pl.col('chrom'),\n",
    "    pl.col('pos').cast(str),\n",
    "    pl.col('ref'),\n",
    "    pl.col('alt'),\n",
    "    pl.lit('hg19')\n",
    "], separator='_').alias('variant_id'))\n",
    "af_hg19 = af_hg19.with_columns(pl.col('transcript_id').str.split('.').list.first().alias('tx_name'))\n",
    "af_hg19.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the ASD/DDD variants to alpha missense\n",
    "missense_variants = pl.from_pandas(final_results.loc[final_results['variant_type']=='missense'].copy())\n",
    "missense_variants = missense_variants.join(af_hg19, on=['variant_id','tx_name'], how='left').drop_nulls().rename({'am_pathogenicity': 'AlphaMissense'})\n",
    "missense_variants.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for codons mutations out of bounds of the max context length\n",
    "context_to_check = 2046\n",
    "checks = check_mutation_positions(missense_variants.to_pandas(), context_to_check)\n",
    "checks[checks['out_of_bounds']].codon_position.hist()\n",
    "plt.title(f' {checks[\"out_of_bounds\"].sum()} out of {len(checks)} variants are out of bounds (context length = {context_to_check})')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.ticker import LogLocator\n",
    "\n",
    "# Count transcripts per variant, then count how many variants fall in each bin\n",
    "counts_pl = missense_variants.group_by('variant_id').count().rename({'count': 'n_transcripts'})\n",
    "hist_pl = (counts_pl\n",
    "           .group_by('n_transcripts')\n",
    "           .count()\n",
    "           .rename({'count': 'n_variants'})\n",
    "           .sort('n_transcripts'))\n",
    "df = hist_pl.to_pandas().astype({'n_transcripts':'int64','n_variants':'int64'})\n",
    "plt.figure(figsize=(20, 4))\n",
    "ax = sns.barplot(data=df, x='n_transcripts', y='n_variants', color='#4c5a88')\n",
    "ax.set_xlabel('Number of transcripts associated with a single variant')\n",
    "ax.set_ylabel('Number of unique variants')\n",
    "ax.set_title('Distribution of transcripts per variant')  \n",
    "ax.set_yscale('log')  \n",
    "ax.yaxis.set_major_locator(LogLocator(base=10)) # 10^k ticks                        # log-scale Y\n",
    "for p in ax.patches:\n",
    "    h = p.get_height()\n",
    "    if h > 0:\n",
    "        ax.annotate(f'{int(h):,}', (p.get_x()+p.get_width()/2, h),\n",
    "                    ha='center', va='bottom', fontsize=9, xytext=(0, 3),\n",
    "                    textcoords='offset points')\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "missense_variants.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "missense_variants.write_csv(f'{OUTPUT_DIR}/ddd_asd_zhouetal_processed_am.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "# 2. ClinVar AlphaMissense Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"alphamissense_data\"\n",
    "variants = pd.read_csv(f'{DATA_DIR}/{dataset_name}/alphamissense_clinvar.csv')\n",
    "variants.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the reference sequence and CDS context annotation of the variants from the GTF annotation and FASTA files\n",
    "# Using same annotation file that the authors used \n",
    "from tqdm import tqdm\n",
    "# Extract assembly from first variant_id (e.g. chr1_925969_C_T_hg38 -> hg38)\n",
    "assembly = variants['variant_id'].iloc[0].split('_')[-1]\n",
    "assert assembly == 'hg38'\n",
    "# Extract genomic coordinates from the variant_id \n",
    "variants[['chrom', 'pos', 'ref', 'alt']] = variants['variant_id'].str.extract(r'(chr\\d+|chrX|chrY)_(\\d+)_([ACGT])_([ACGT])')\n",
    "variants['pos'] = variants['pos'].astype(int)\n",
    "variants = variants.sort_values(by=['chrom','pos']).reset_index(drop=True).reset_index()\n",
    "# Remove version numbers after dot in transcript_id\n",
    "variants['transcript_id'] = variants['transcript_id'].str.split('.').str[0]\n",
    "gtf_s, fasta = process_gtf(f'{DATA_DIR}/ucsc_gencodev32_hg38.tsv', f'{DATA_DIR}/reference/{assembly}/{assembly}.fa')\n",
    "print(f\"Processed {gtf_s.shape[0]} GTF CDS sequences\")\n",
    "display(gtf_s[['name', 'chrom', 'strand', 'cdsStart', 'cdsEnd', 'cds_starts', 'cds_ends', 'cds_length']].head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process variants per chromosome\n",
    "all_results = []\n",
    "# Filter sequences with CDS length not multiple of 3\n",
    "gtf_s = gtf_s[gtf_s['cds_length'].astype(int) % 3 == 0]\n",
    "chroms = variants['chrom'].unique()\n",
    "for chrom in tqdm(chroms, desc=\"Processing chromosomes\", total=len(chroms)):\n",
    "    curr_variants = variants[variants['chrom']==chrom][['variant_id', 'chrom','pos','ref','alt']].drop_duplicates().copy()\n",
    "    chrom_gtf = gtf_s[gtf_s['chrom']==chrom]\n",
    "    chrom_results = process_a_chrom(curr_variants, chrom_gtf, return_alt_cds=True)\n",
    "    all_results.append(chrom_results)\n",
    "all_results = pd.concat(all_results).reset_index(drop=True)\n",
    "all_results['variant_id'] = all_results['variant_id'] + '_' + assembly\n",
    "all_results = all_results.merge(variants.drop('variant_id', axis=1), left_on=['chrom','pos','ref','alt'], \n",
    "                                right_on=['chrom','pos','ref','alt'])\n",
    "print(f\"\\n Processed {all_results.shape[0]} mutations with CDS context:\")\n",
    "cols_to_display = ['chrom', 'pos', 'variant_id', 'ref', 'alt', 'tx_name', 'cdsStart',\n",
    "       'cdsEnd', 'tx_strand', 'var_rel_dist_in_cds', 'ref_codon',\n",
    "       'alt_codon', 'ref_aa', 'alt_aa', 'codon_position', 'index',\n",
    "       'transcript_id', 'protein_variant', 'AlphaMissense', 'label']\n",
    "display(all_results[cols_to_display].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure reference allele in fasta matches reference allele in variants\n",
    "for i in range(variants.shape[0]):\n",
    "    t = variants.iloc[i]\n",
    "    variant_id = t['variant_id']\n",
    "    chrom = t['chrom']\n",
    "    pos = t['pos']\n",
    "    ref = t['ref']\n",
    "    hg_ref = fasta[chrom][pos-1]\n",
    "    if hg_ref != ref:\n",
    "        print(f\"Mismatch at {chrom}:{pos}, {ref} != {hg_ref}, {variant_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge variants with processed results based on transcript_id\n",
    "merged = variants.merge(all_results, left_on=['transcript_id','variant_id','chrom','pos','ref','alt'], \n",
    "                            right_on=['tx_name','variant_id','chrom','pos','ref','alt'], \n",
    "                        suffixes=('', '_y'))\n",
    "merged.rename(columns={'index': 'id'}, inplace=True)\n",
    "merged[['label']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_transcript_distribution(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_csv(f'{OUTPUT_DIR}/alphamissense_clinvar_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "# 3. Cancer Hotspot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"alphamissense_data\"\n",
    "variants = pd.read_csv(f'{DATA_DIR}/{dataset_name}/alphamissense_cancer_hotspot.csv')\n",
    "variants.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the reference sequence and CDS context annotation of the variants from the GTF annotation and FASTA files #\n",
    "# Using same annotation file that the authors used \n",
    "\n",
    "## Extract assembly from first variant_id (e.g. chr1_925969_C_T_hg38 -> hg38)\n",
    "assembly = variants['variant_id'].iloc[0].split('_')[-1]\n",
    "assert assembly == 'hg38'\n",
    "## Extract genomic coordinates from the variant_id \n",
    "variants[['chrom', 'pos', 'ref', 'alt']] = variants['variant_id'].str.extract(r'(chr\\d+|chrX|chrY)_(\\d+)_([ACGT])_([ACGT])')\n",
    "variants['pos'] = variants['pos'].astype(int)\n",
    "variants = variants.sort_values(by=['chrom','pos']).reset_index(drop=True).reset_index()\n",
    "## Remove version numbers after dot in transcript_id\n",
    "variants['transcript_id'] = variants['transcript_id'].str.split('.').str[0]\n",
    "## Get the CDS sequences and annotations from the GTF and FASTA files\n",
    "gtf_s, fasta = process_gtf(f'{DATA_DIR}/ucsc_gencodev32_hg38.tsv', f'{DATA_DIR}/reference/{assembly}/{assembly}.fa')\n",
    "print(f\"Processed {gtf_s.shape[0]} GTF CDS sequences\")\n",
    "display(gtf_s[['name', 'chrom', 'strand', 'cdsStart', 'cdsEnd', 'cds_starts', 'cds_ends', 'cds_length']].head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process variants per chromosome\n",
    "all_results = []\n",
    "# Filter sequences with CDS length not multiple of 3\n",
    "gtf_s = gtf_s[gtf_s['cds_length'].astype(int) % 3 == 0]\n",
    "chroms = variants['chrom'].unique()\n",
    "for chrom in tqdm(chroms, desc=\"Processing chromosomes\", total=len(chroms)):\n",
    "    curr_variants = variants[variants['chrom']==chrom][['variant_id', 'chrom','pos','ref','alt']].drop_duplicates().copy()\n",
    "    chrom_gtf = gtf_s[gtf_s['chrom']==chrom]\n",
    "    chrom_results = process_a_chrom(curr_variants, chrom_gtf, return_alt_cds=True)\n",
    "    all_results.append(chrom_results)\n",
    "all_results = pd.concat(all_results).reset_index(drop=True)\n",
    "all_results['variant_id'] = all_results['variant_id'] + '_' + assembly\n",
    "all_results = all_results.merge(variants.drop('variant_id', axis=1), left_on=['chrom','pos','ref','alt'], \n",
    "                                right_on=['chrom','pos','ref','alt'])\n",
    "print(f\"\\n Processed {all_results.shape[0]} mutations with CDS context:\")\n",
    "cols_to_display = ['chrom', 'pos', 'variant_id', 'ref', 'alt', 'tx_name', 'cdsStart',\n",
    "       'cdsEnd', 'tx_strand', 'var_rel_dist_in_cds', 'ref_codon',\n",
    "       'alt_codon', 'ref_aa', 'alt_aa', 'codon_position', 'index',\n",
    "       'transcript_id', 'protein_variant', 'AlphaMissense', 'label']\n",
    "display(all_results[cols_to_display].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure reference allele in fasta matches reference allele in variants\n",
    "for i in range(variants.shape[0]):\n",
    "    t = variants.iloc[i]\n",
    "    variant_id = t['variant_id']\n",
    "    chrom = t['chrom']\n",
    "    pos = t['pos']\n",
    "    ref = t['ref']\n",
    "    hg_ref = fasta[chrom][pos-1]\n",
    "    if hg_ref != ref:\n",
    "        print(f\"Mismatch at {chrom}:{pos}, {ref} != {hg_ref}, {variant_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = variants.merge(all_results, left_on=['transcript_id','variant_id','chrom','pos','ref','alt'], \n",
    "                            right_on=['tx_name','variant_id','chrom','pos','ref','alt'], \n",
    "                        suffixes=('', '_y'))\n",
    "merged.rename(columns={'index': 'id'}, inplace=True)\n",
    "merged.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_transcript_distribution(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_csv(f'{OUTPUT_DIR}/alphamissense_cancer_hotspot_processed.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "# 4. ClinVar Synonymous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"clinvar_synom\"\n",
    "\n",
    "# Get the synonymous variants from Clinvar\n",
    "data = pd.read_table(f'{DATA_DIR}/clinvar_syn/variant_summary.txt.gz', low_memory=False)\n",
    "data.iloc[:, 18] = data.iloc[:,18].astype(str)\n",
    "data = pl.from_pandas(data)\n",
    "data = data.rename({'#AlleleID': 'AlleleID'})\n",
    "data = data.filter((pl.col('Type') == 'single nucleotide variant') &\n",
    "                   (pl.col('Assembly') == 'GRCh38') &\n",
    "                   (pl.col('ReviewStatus').is_in(['practice guideline',\n",
    "                                                  'reviewed by expert panel',\n",
    "                                                  'criteria provided, multiple submitters, no conflicts',\n",
    "                                                  ])))\n",
    "# normalize ref and alt                                                   \n",
    "data = data.with_columns(('chr' + pl.col('Chromosome')).alias('chrom'),\n",
    "                               pl.col('PositionVCF').alias('pos'),\n",
    "                               pl.col('ReferenceAlleleVCF').alias('ref'),\n",
    "                               pl.col('AlternateAlleleVCF').alias('alt'))\n",
    "\n",
    "# filter synonymous variants\n",
    "valid_chroms = ['chr' + str(i) for i in range(1, 23)] \n",
    "clinvar_syn = data.filter(pl.col('Name').str.ends_with('=)')).filter(pl.col('ref') != pl.col('alt'))\n",
    "clinvar_syn = clinvar_syn.filter(pl.col('chrom').is_in(valid_chroms))\n",
    "\n",
    "# Label variants as benign or pathogenic\n",
    "benign_labels = ['Benign', 'Likely benign', 'Benign/Likely benign', ]\n",
    "patho_labels = ['Likely pathogenic', 'Pathogenic', 'Pathogenic/Likely pathogenic']\n",
    "clinvar_syn = clinvar_syn.filter(pl.col('ClinicalSignificance').is_in(benign_labels + patho_labels))\n",
    "\n",
    "\n",
    "clinvar_syn.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the reference sequence and CDS context annotation of the variants from the GTF and FASTA files\n",
    "refseq = pl.read_csv(f'{DATA_DIR}/clinvar_syn/ucsc_refseq_hg38.tsv', separator='\\t')\n",
    "refseq.head(2)\n",
    "\n",
    "# Build CDS sequences for synonymous variants\n",
    "valid_chroms = ['chr' + str(i) for i in range(1, 23)] \n",
    "refseq = pl.read_csv(f'{DATA_DIR}/clinvar_syn/ucsc_refseq_hg38.tsv', separator='\\t')\n",
    "refseq_hist = pl.read_csv(f'{DATA_DIR}/clinvar_syn/ucsc_refseq_hist_hg38.tsv', separator='\\t')\n",
    "refseq = pl.concat([refseq, refseq_hist])\n",
    "refseq = refseq.filter(pl.col('chrom').is_in(valid_chroms)).unique()\n",
    "fasta = {}\n",
    "\n",
    "with pyfaidx.Fasta(f'{DATA_DIR}/reference/hg38/hg38.fa') as f:\n",
    "    for chrom in refseq['chrom'].unique():\n",
    "        fasta[chrom] = f[chrom][:].seq\n",
    "refseq = refseq.with_columns(\n",
    "    pl.struct(pl.all()).map_elements(lambda row: extract_cds_sequence(row, fasta), return_dtype=pl.String).alias('cds_sequence')\n",
    ").filter(pl.col('cds_sequence').str.len_chars() %3==0)\n",
    "refseq.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add splicing junction information\n",
    "splicing_juncs = []\n",
    "for row in refseq.iter_rows(named=True):\n",
    "    exon_starts = row['exonStarts'].strip(',').split(',')\n",
    "    exon_ends = row['exonEnds'].strip(',').split(',')\n",
    "    exon_starts = [int(i) for i in exon_starts]\n",
    "    exon_ends = [int(i) for i in exon_ends]\n",
    "    for i in range(1, len(exon_starts)):\n",
    "        splicing_juncs.append([row['name'], row['chrom'], row['strand'], exon_starts[i]-2, exon_starts[i] + 2])\n",
    "    for i in range(0, len(exon_ends)-1):\n",
    "        splicing_juncs.append([row['name'], row['chrom'], row['strand'], exon_ends[i]-2, exon_ends[i] + 2])\n",
    "splicing_juncs = pd.DataFrame(splicing_juncs, columns=['tx', 'chrom', 'strand', 'start', 'end'])\n",
    "splicing_juncs = pl.from_pandas(splicing_juncs)\n",
    "\n",
    "# Extract all unique transcripts first\n",
    "unique_tx = clinvar_syn['Name'].str.split(':').list.get(0).str.split('(').list.get(0).unique()\n",
    "# Create a dictionary mapping transcripts to their junction ranges\n",
    "tx_junc_dict = {}\n",
    "for tx in unique_tx:\n",
    "    tx_juncs = splicing_juncs.filter(pl.col('tx') == tx)\n",
    "    if len(tx_juncs) > 0:\n",
    "        tx_junc_dict[tx] = {\n",
    "            'starts': tx_juncs['start'].to_numpy(),\n",
    "            'ends': tx_juncs['end'].to_numpy()\n",
    "        }\n",
    "\n",
    "def check_splice_junction_fast(variant):\n",
    "    tx = variant['Name'].split(':')[0].split('(')[0]\n",
    "    # if tx not in tx_junc_dict:\n",
    "    #     return False\n",
    "    juncs = tx_junc_dict[tx]\n",
    "    return np.any((variant['pos']-1 >= juncs['starts']) & (variant['pos']-1 < juncs['ends']))\n",
    "\n",
    "tx_gposes = {}\n",
    "for row in refseq.rows(named=True):\n",
    "    curr_poses = []\n",
    "    chrom = row['chrom']\n",
    "    strand = row['strand']\n",
    "    cds_start = row['cdsStart']\n",
    "    cds_end = row['cdsEnd']\n",
    "    \n",
    "    # Parse exon coordinates\n",
    "    exon_starts = [int(x) for x in row['exonStarts'].rstrip(',').split(',')]\n",
    "    exon_ends = [int(x) for x in row['exonEnds'].rstrip(',').split(',')]\n",
    "    for start, end in zip(exon_starts, exon_ends):\n",
    "        # Find overlap between exon and CDS\n",
    "        overlap_start = max(start, cds_start)\n",
    "        overlap_end = min(end, cds_end)\n",
    "        for i in range(overlap_start, overlap_end):\n",
    "            curr_poses.append(i)\n",
    "    if strand == '-':\n",
    "        curr_poses = curr_poses[::-1]\n",
    "    tx_gposes[(row['chrom'], row['name'])] = curr_poses\n",
    "\n",
    "dset = clinvar_syn.with_columns(pl.col('Name').str.split(':').list.get(0).str.split('(').list.get(0).alias('tx'))\n",
    "dset = dset.filter(pl.col('tx').is_in(tx_junc_dict.keys()))\n",
    "dset = dset.with_columns(pl.struct(pl.all()).map_elements(check_splice_junction_fast, return_dtype=pl.Boolean).alias('in_splice_junction'))\n",
    "\n",
    "dset.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process variants per chromosome and add additional features: pLI, PhyloP, codon frequencies\n",
    "import re\n",
    "result = []\n",
    "\n",
    "for row in tqdm(dset.rows(named=True)):\n",
    "    s = row['Name'].split(':')[1].split(' ')[0]\n",
    "    m = re.fullmatch(r'c\\.(\\d+)([ACGT])>([ACGT])', s)\n",
    "    pos_cds, ref_cds, alt_cds = int(m.group(1)), m.group(2), m.group(3)\n",
    "\n",
    "    tx = refseq.filter((pl.col('name')==row['tx']) & (pl.col('chrom')==row['chrom']))[0]\n",
    "    try:\n",
    "        pos_cds0 = tx_gposes[(row['chrom'], row['tx'])].index(row['pos']-1)\n",
    "    except:\n",
    "        continue\n",
    "    seq = tx[0,'cds_sequence']\n",
    "    if pos_cds0+1 != pos_cds:\n",
    "        print(str(row))\n",
    "    assert seq[pos_cds0] == ref_cds\n",
    "    assert ref_cds == row['ref'] if tx[0,'strand'] == '+' else get_reverse_complement(row['ref'])\n",
    "    assert alt_cds == row['alt'] if tx[0,'strand'] == '+' else get_reverse_complement(row['alt'])\n",
    "\n",
    "    codon_position = pos_cds0 // 3\n",
    "    ref_codon = seq[codon_position*3:(codon_position+1)*3]\n",
    "    remainder = pos_cds0 % 3\n",
    "    alt_nuc = list(ref_codon)\n",
    "    alt_nuc[remainder] = alt_cds\n",
    "    alt_codon = ''.join(alt_nuc)\n",
    "    item = {\n",
    "        'chrom': row['chrom'],\n",
    "        'pos': row['pos'],\n",
    "        'ref': row['ref'],\n",
    "        'alt': row['alt'],\n",
    "        'var_rel_dist_in_cds': pos_cds0,\n",
    "        'codon_position': codon_position,\n",
    "        'ref_codon': ref_codon,\n",
    "        'alt_codon': alt_codon,\n",
    "        'tx': row['tx'],\n",
    "        'label': row['ClinicalSignificance'],\n",
    "        'in_splice_junction': row['in_splice_junction'],\n",
    "        'ref_seq': seq,\n",
    "        'alt_seq': seq[:pos_cds0] + alt_cds + seq[pos_cds0+1:],\n",
    "        \n",
    "    }\n",
    "    result.append(item)\n",
    "\n",
    "\n",
    "result_df = pl.from_dicts(result).with_row_index('id')\n",
    "frame = result_df.to_pandas()\n",
    "(frame[\"ref_seq\"].apply(lambda x: len(x) == 0)).sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Adding additional features (pLI, PhyloP, codon frequencies)...')\n",
    "dset = process_dset(result_df, refseq, remove_non_pli=False)\n",
    "print(f'Dataset with additional features: {dset.shape[0]} variants')\n",
    "dset.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_to_check = 2046\n",
    "checks = check_mutation_positions(result_df.to_pandas(), context_to_check)\n",
    "checks[checks['out_of_bounds']].codon_position.hist(figsize=(5, 5))\n",
    "plt.title(f' {checks[\"out_of_bounds\"].sum()} out of {len(checks)} variants are out of bounds (context length = {context_to_check})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed results, dset, and refseq tables\n",
    "dset.write_csv(f'{OUTPUT_DIR}/clinvar_synom.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
