{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Tutorial 4: RiboNN Translation Efficiency Prediction with Encodon\n",
    "\n",
    "This notebook demonstrates predicting translation efficiency using pretrained Encodon models.\n",
    "\n",
    "## Overview\n",
    "- **Task**: Predict translation efficiency from mRNA sequences\n",
    "- **Dataset**: RiboNN dataset with human translation efficiency data (see [1])\n",
    "- **Model**: Pretrained Encodon + Random Forest regressor\n",
    "\n",
    "[1] Zheng, Dinghai, et al. \"Predicting the translation efficiency of messenger RNA in mammalian cells.\" Nature biotechnology (2025): 1-14."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add project paths\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import Encodon modules\n",
    "from src.inference.encodon import EncodonInference\n",
    "from src.inference.task_types import TaskTypes\n",
    "from src.data.metadata import MetadataFields\n",
    "\n",
    "# Fix random seed\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 2. Load Pretrained Encodon Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define checkpoint paths\n",
    "\n",
    "ckpt_path = \"./\"\n",
    "AVAILABLE_MODELS = [\n",
    "    f\"{ckpt_path}/NV-CodonFM-Encodon-80M-v1/NV-CodonFM-Encodon-80M-v1.safetensors\",\n",
    "    f\"{ckpt_path}/NV-CodonFM-Encodon-600M-v1/NV-CodonFM-Encodon-600M-v1.safetensors\",\n",
    "    f\"{ckpt_path}/NV-CodonFM-Encodon-1B-v1/NV-CodonFM-Encodon-1B-v1.safetensors\",\n",
    "    f\"{ckpt_path}/NV-CodonFM-Encodon-Cdwt-1B-v1/NV-CodonFM-Encodon-Cdwt-1B-v1.safetensors\"\n",
    "]\n",
    "checkpoint_path = AVAILABLE_MODELS[0]\n",
    "\n",
    "model_loaded = False\n",
    "if os.path.exists(checkpoint_path):\n",
    "    try:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        # Create EncodonInference wrapper\n",
    "        encodon_model = EncodonInference(\n",
    "            model_path=checkpoint_path,\n",
    "            task_type=TaskTypes.EMBEDDING_PREDICTION,\n",
    "        )\n",
    "        \n",
    "        # Configure model\n",
    "        encodon_model.configure_model()\n",
    "        encodon_model.to(device)\n",
    "        encodon_model.eval()\n",
    "        \n",
    "        print(f\"✅ Model loaded from: {checkpoint_path}\")\n",
    "        print(f\"Device: {device}\")\n",
    "        print(f\"Parameters: {sum(p.numel() for p in encodon_model.model.parameters()):,}\")\n",
    "        \n",
    "        model_loaded = True        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {checkpoint_path}: {e}\")\n",
    "\n",
    "if not model_loaded:\n",
    "    print(\"❌ Could not load any model. Please check checkpoint paths.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 3. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "# Configurable dataset path\n",
    "data_path = \"/data/validation/processed/data_with_human_TE_cellline_all_NA_plain.csv\"\n",
    "\n",
    "# Source URL for the TE dataset\n",
    "te_dataset_url = \"https://raw.githubusercontent.com/CenikLab/TE_classic_ML/refs/heads/main/data/data_with_human_TE_cellline_all_NA_plain.csv\"\n",
    "\n",
    "# Ensure parent directory exists\n",
    "Path(os.path.dirname(data_path)).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Download if missing\n",
    "if not os.path.exists(data_path):\n",
    "    print(f\"Downloading TE dataset to {data_path} ...\")\n",
    "    urllib.request.urlretrieve(te_dataset_url, data_path)\n",
    "    print(\"Download complete.\")\n",
    "else:\n",
    "    print(f\"Found existing dataset at {data_path}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data = pl.read_csv(data_path, separator='\\t')\n",
    "data = data.with_columns([\n",
    "            pl.struct(['utr5_size', 'cds_size', 'tx_sequence']).map_elements(\n",
    "                lambda row: row['tx_sequence'][row['utr5_size']:row['utr5_size'] + row['cds_size']] ,\n",
    "                return_dtype=pl.Utf8\n",
    "            ).alias('cds_sequence'),\n",
    "            pl.struct(['utr5_size', 'tx_sequence']).map_elements(\n",
    "                lambda row: row['tx_sequence'][:row['utr5_size']],\n",
    "                return_dtype=pl.Utf8\n",
    "            ).alias('utr5_sequence'),\n",
    "            pl.struct(['utr5_size', 'cds_size', 'tx_sequence']).map_elements(\n",
    "                lambda row: row['tx_sequence'][row['utr5_size'] + row['cds_size']:],\n",
    "                return_dtype=pl.Utf8\n",
    "            ).alias('utr3_sequence')\n",
    "        ]).with_row_index('id')\n",
    "output_path = data_path[:-4] + '.processed.csv'\n",
    "data.write_csv(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load RiboNN dataset\n",
    "data_loaded = False\n",
    "if os.path.exists(output_path):\n",
    "    try:\n",
    "        data = pl.read_csv(output_path)\n",
    "        print(f\"✅ Loaded {len(data)} sequences from: {output_path}\")\n",
    "        print(f\"Shape: {data.shape}\")\n",
    "        print(f\"Key columns: {[col for col in ['id', 'cds_sequence', 'mean_te', 'fold'] if col in data.columns]}\")\n",
    "        \n",
    "        data_loaded = True\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {output_path}: {e}\")\n",
    "\n",
    "    # Show basic statistics\n",
    "    te_stats = data.select([\n",
    "        pl.col('mean_te').mean().alias('mean'),\n",
    "        pl.col('mean_te').std().alias('std'),\n",
    "        pl.col('mean_te').min().alias('min'),\n",
    "        pl.col('mean_te').max().alias('max')\n",
    "    ])\n",
    "    print(f\"\\nTranslation Efficiency stats:\")\n",
    "    print(f\"  Mean: {te_stats['mean'][0]:.4f}\")\n",
    "    print(f\"  Range: [{te_stats['min'][0]:.4f}, {te_stats['max'][0]:.4f}]\")\n",
    "    data_loaded = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_size = 500 # set to len(data) for full dataset\n",
    "batch_size = 16\n",
    "\n",
    "# Subsample data while maintaining split proportions using sklearn\n",
    "if data_loaded and demo_size < len(data):\n",
    "    print(f\"=== SUBSAMPLING DATA ===\")\n",
    "    sample_fraction = demo_size / len(data)\n",
    "    _, data = train_test_split(\n",
    "        data, \n",
    "        test_size=sample_fraction,\n",
    "        stratify=data['fold'],\n",
    "        random_state=42\n",
    "    )\n",
    "else:\n",
    "    print(f\"Using full dataset: {len(data) if data_loaded else 0} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_size = 500 # set to len(data) for full dataset\n",
    "batch_size = 16\n",
    "if data_loaded and model_loaded:\n",
    "    print(\"=== DATA PREPROCESSING ===\")\n",
    "    data = data.to_pandas()\n",
    "    sequences = data['cds_sequence'].tolist()\n",
    "    targets = data['mean_te'].values    \n",
    "    # Use subset for demo\n",
    "    sequences = sequences\n",
    "    targets = targets\n",
    "    \n",
    "    print(f\"Processing {demo_size} sequences (demo mode)\")\n",
    "    \n",
    "    # Extract embeddings\n",
    "    print(\"\\nExtracting embeddings...\")\n",
    "    all_embeddings = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(sequences), batch_size)):\n",
    "        batch_seqs = sequences[i:i + batch_size]\n",
    "        \n",
    "        # Prepare batch\n",
    "        batch_items = []\n",
    "        for j, seq in enumerate(batch_seqs):\n",
    "            seq = seq.upper().replace('U', 'T')\n",
    "            tokens = encodon_model.tokenizer.tokenize(seq)\n",
    "            input_ids = encodon_model.tokenizer.convert_tokens_to_ids(tokens)\n",
    "            \n",
    "            # Truncate if needed\n",
    "            if len(input_ids) > encodon_model.model.hparams.max_position_embeddings - 2:  # Leave room for CLS/SEP\n",
    "                input_ids = input_ids[:encodon_model.model.hparams.max_position_embeddings - 2]\n",
    "            \n",
    "            # Add special tokens\n",
    "            input_ids = [encodon_model.tokenizer.cls_token_id] + input_ids + [encodon_model.tokenizer.sep_token_id]\n",
    "            attention_mask = [1] * len(input_ids)\n",
    "            \n",
    "            batch_items.append({\n",
    "                MetadataFields.INPUT_IDS: input_ids,\n",
    "                MetadataFields.ATTENTION_MASK: attention_mask,\n",
    "            })\n",
    "        \n",
    "        # Pad batch\n",
    "        max_len = encodon_model.model.hparams.max_position_embeddings\n",
    "        \n",
    "        padded_input_ids = []\n",
    "        padded_attention_masks = []\n",
    "        batch_ids = []\n",
    "        \n",
    "        for item in batch_items:\n",
    "            input_ids = item[MetadataFields.INPUT_IDS]\n",
    "            attention_mask = item[MetadataFields.ATTENTION_MASK]\n",
    "            \n",
    "            # Pad\n",
    "            pad_len = max_len - len(input_ids)\n",
    "            input_ids.extend([encodon_model.tokenizer.pad_token_id] * pad_len)\n",
    "            attention_mask.extend([0] * pad_len)\n",
    "            \n",
    "            padded_input_ids.append(input_ids)\n",
    "            padded_attention_masks.append(attention_mask)\n",
    "        \n",
    "        # Create batch tensor\n",
    "        batch = {\n",
    "            MetadataFields.INPUT_IDS: torch.tensor(padded_input_ids, dtype=torch.long).to(encodon_model.device),\n",
    "            MetadataFields.ATTENTION_MASK: torch.tensor(padded_attention_masks, dtype=torch.long).to(encodon_model.device),\n",
    "        }\n",
    "        \n",
    "        # Extract embeddings\n",
    "        output = encodon_model.extract_embeddings(batch)\n",
    "        all_embeddings.append(output.embeddings)\n",
    "    \n",
    "    # Combine embeddings\n",
    "    embeddings = np.vstack(all_embeddings)\n",
    "    print(f\"\\n✅ Extracted embeddings: {embeddings.shape}\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Skipping preprocessing - data or model not loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 5. Train Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'embeddings' in locals() and 'targets' in locals():\n",
    "    print(\"=== TRAINING RANDOM FOREST ===\")\n",
    "    results = {'r2_scores': [], 'pearson_scores': [], 'mse_scores': []}\n",
    "    folds = data['fold'].unique()\n",
    "    for fold in folds:\n",
    "        train_idx = data[data['fold'] != fold].index\n",
    "        test_idx = data[data['fold'] == fold].index\n",
    "        X_train, X_test = embeddings[train_idx], embeddings[test_idx]\n",
    "        y_train, y_test = targets[train_idx], targets[test_idx]\n",
    "        \n",
    "        # Train Random Forest\n",
    "        rf = RandomForestRegressor(\n",
    "            n_estimators=500,\n",
    "            max_depth=15,\n",
    "            min_samples_split=2,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        rf.fit(X_train, y_train)\n",
    "        y_pred = rf.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        pearson_r, _ = pearsonr(y_test, y_pred)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        \n",
    "        results['r2_scores'].append(r2)\n",
    "        results['pearson_scores'].append(pearson_r)\n",
    "        results['mse_scores'].append(mse)\n",
    "        \n",
    "        print(f\"Fold {fold}: R² = {r2:.4f}, r = {pearson_r:.4f}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    mean_r2 = np.mean(results['r2_scores'])\n",
    "    mean_pearson = np.mean(results['pearson_scores'])\n",
    "    mean_mse = np.mean(results['mse_scores'])\n",
    "    \n",
    "    print(f\"\\n=== CROSS-VALIDATION RESULTS ===\")\n",
    "    print(f\"Mean R²: {mean_r2:.4f} ± {np.std(results['r2_scores']):.4f}\")\n",
    "    print(f\"Mean Pearson r: {mean_pearson:.4f} ± {np.std(results['pearson_scores']):.4f}\")\n",
    "    print(f\"Mean RMSE: {np.sqrt(mean_mse):.4f}\")\n",
    "    \n",
    "    # Store final model trained on all data\n",
    "    final_rf = RandomForestRegressor(n_estimators=1000, max_depth=5, random_state=42, n_jobs=-1)\n",
    "    final_rf.fit(embeddings, targets)\n",
    "    final_predictions = final_rf.predict(embeddings)\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Cannot train - missing embeddings or targets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## 6. Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'final_predictions' in locals():\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 8))\n",
    "    fig.suptitle('RiboNN Translation Efficiency Prediction Results', fontsize=16)\n",
    "    \n",
    "    \n",
    "    # Cross-validation performance\n",
    "    axes[0].plot(range(len(results['r2_scores'])), results['r2_scores'], 'o-', label='R²')\n",
    "    axes[0].plot(range(len(results['pearson_scores'])), results['pearson_scores'], 's-', label='Pearson r')\n",
    "    axes[0].set_xlabel('Fold')\n",
    "    axes[0].set_ylabel('Score')\n",
    "    axes[0].set_title('Cross-Validation Performance')\n",
    "    \n",
    "    # Add mean performance lines\n",
    "    mean_r2 = np.mean(results['r2_scores'])\n",
    "    mean_pearson = np.mean(results['pearson_scores'])\n",
    "    axes[0].axhline(mean_r2, color='blue', linestyle='--', alpha=0.7, label=f'Mean R² = {mean_r2:.3f}')\n",
    "    axes[0].axhline(mean_pearson, color='orange', linestyle='--', alpha=0.7, label=f'Mean Pearson r = {mean_pearson:.3f}')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Target distribution\n",
    "    axes[1].hist(targets, bins=30, alpha=0.7, edgecolor='black')\n",
    "    axes[1].axvline(targets.mean(), color='red', linestyle='--', label=f'Mean = {targets.mean():.3f}')\n",
    "    axes[1].set_xlabel('Translation Efficiency')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].set_title('Target Distribution')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"❌ No results to plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## 7. Troubleshooting & Optimization Tips\n",
    "\n",
    "### Common Issues and Solutions:\n",
    "\n",
    "#### 1. Model Loading Issues\n",
    "- **Problem**: Checkpoint not found\n",
    "- **Solution**: Update checkpoint paths in section 2\n",
    "- **Check**: Verify checkpoint files exist and are accessible\n",
    "\n",
    "#### 2. Data Loading Issues\n",
    "- **Problem**: Dataset not found\n",
    "- **Solution**: Update data paths in section 3\n",
    "- **Check**: Ensure CSV files have required columns (id, ref_seq, value)\n",
    "\n",
    "#### 3. Memory Issues\n",
    "- **Problem**: CUDA out of memory\n",
    "- **Solution**: Reduce batch_size in preprocessing section\n",
    "- **Alternative**: Use CPU by setting device='cpu'\n",
    "\n",
    "#### 4. Performance Issues\n",
    "- **Problem**: Low R² scores\n",
    "- **Solutions**:\n",
    "  - Try larger models (600M or 1B parameters)\n",
    "  - Implement fine-tuning instead of just embeddings\n",
    "  - Tune Random Forest hyperparameters\n",
    "  - Check data quality and preprocessing\n",
    "\n",
    "\n",
    "### Optimization Strategies:\n",
    "\n",
    "#### 1. Model Architecture\n",
    "- **80M model**: Fast, good for initial experiments\n",
    "- **600M model**: Better performance, moderate cost\n",
    "- **1B model**: Best performance, highest computational cost\n",
    "\n",
    "#### 3. Hyperparameter Tuning\n",
    "```python\n",
    "# Try these Random Forest parameters:\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [10, 15, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
