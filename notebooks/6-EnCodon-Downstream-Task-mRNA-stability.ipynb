{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Tutorial 6: mRNA Stability Prediction with Encodon\n",
    "\n",
    "This notebook demonstrates predicting mRNA stability using pretrained Encodon models.\n",
    "\n",
    "## Overview\n",
    "- **Task**: Predict mRNA stability/degradation from sequences\n",
    "- **Dataset**: mRNA Stability dataset (see [1])\n",
    "- **Model**: Pretrained Encodon + Random Forest regressor\n",
    "\n",
    "[1] Li, Sizhen, et al. \"CodonBERT large language model for mRNA vaccines.\" Genome research 34.7 (2024): 1027-1035."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add project paths\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import Encodon modules\n",
    "from src.inference.encodon import EncodonInference\n",
    "from src.inference.task_types import TaskTypes\n",
    "from src.data.metadata import MetadataFields\n",
    "\n",
    "# Fix random seed\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "from src.data.codon_bert_dataset import CodonBertDataset\n",
    "from src.data.preprocess.codon_sequence import process_item\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 2. Load Pretrained Encodon Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define checkpoint paths\n",
    "ckpt_path = \"./\"\n",
    "AVAILABLE_MODELS = [\n",
    "    f\"{ckpt_path}/NV-CodonFM-Encodon-80M-v1/NV-CodonFM-Encodon-80M-v1.safetensors\",\n",
    "    f\"{ckpt_path}/NV-CodonFM-Encodon-600M-v1/NV-CodonFM-Encodon-600M-v1.safetensors\",\n",
    "    f\"{ckpt_path}/NV-CodonFM-Encodon-1B-v1/NV-CodonFM-Encodon-1B-v1.safetensors\",\n",
    "    f\"{ckpt_path}/NV-CodonFM-Encodon-Cdwt-1B-v1/NV-CodonFM-Encodon-Cdwt-1B-v1.safetensors\"\n",
    "]\n",
    "checkpoint_path = AVAILABLE_MODELS[0]\n",
    "\n",
    "model_loaded = False\n",
    "if os.path.exists(checkpoint_path):\n",
    "    try:\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        # Create EncodonInference wrapper\n",
    "        encodon_model = EncodonInference(\n",
    "            model_path=checkpoint_path,\n",
    "            task_type=TaskTypes.EMBEDDING_PREDICTION,\n",
    "        )\n",
    "        \n",
    "        # Configure model\n",
    "        encodon_model.configure_model()\n",
    "        encodon_model.to(device)\n",
    "        encodon_model.eval()\n",
    "        \n",
    "        print(f\"‚úÖ Model loaded from: {checkpoint_path}\")\n",
    "        print(f\"Device: {device}\")\n",
    "        print(f\"Parameters: {sum(p.numel() for p in encodon_model.model.parameters()):,}\")\n",
    "        \n",
    "        model_loaded = True        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {checkpoint_path}: {e}\")\n",
    "\n",
    "if not model_loaded:\n",
    "    print(\"‚ùå Could not load any model. Please check checkpoint paths.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 3. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download mRNA Stability dataset if it doesn't exist\n",
    "import subprocess\n",
    "\n",
    "# NOTE: This assumes the notebook was launched from the codon-fm source directory.\n",
    "# NOTE: otherwise change the path for the `subprocess` launch to correspond to the data_scripts path correctly\n",
    "\n",
    "root_path = \"/data/validation/processed\"\n",
    "data_path = \"/data/validation/processed/mRNA_Stability.csv\"\n",
    "if not os.path.exists(data_path):\n",
    "    print(\"üì• Downloading mRNA Stability dataset...\")\n",
    "    try:\n",
    "        subprocess.run([\n",
    "            \"python\", \"data_scripts/download_preprocess_codonbert_bench.py\",\n",
    "            \"--dataset\", \"mRNA_Stability.csv\",\n",
    "            \"--output-dir\", root_path\n",
    "        ], check=True)\n",
    "        print(\"‚úÖ Dataset downloaded and preprocessed successfully!\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ùå Error downloading dataset: {e}\")\n",
    "        print(\"Please ensure the data_scripts are available and run manually if needed.\")\n",
    "else:\n",
    "    print(\"‚úÖ Dataset already exists!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mRNA Stability dataset\n",
    "data_loaded = False\n",
    "if os.path.exists(data_path):\n",
    "    try:\n",
    "        data = pd.read_csv(data_path)\n",
    "        print(f\"‚úÖ Loaded {len(data)} samples from: {data_path}\")\n",
    "        print(f\"Columns: {list(data.columns)}\")\n",
    "        \n",
    "        if 'split' in data.columns:\n",
    "            print(f\"Data splits: {data['split'].value_counts().to_dict()}\")\n",
    "        \n",
    "        print(f\"Target range: [{data['value'].min():.3f}, {data['value'].max():.3f}]\")\n",
    "        data_loaded = True\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {data_path}: {e}\")\n",
    "\n",
    "if not data_loaded:\n",
    "    print(\"‚ùå Could not load mRNA Stability data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "if data_loaded and model_loaded:\n",
    "    print(\"=== DATA PREPROCESSING ===\")    \n",
    "    # Create dataset\n",
    "    dataset = CodonBertDataset(\n",
    "        data_path=data_path,\n",
    "        tokenizer=encodon_model.tokenizer,\n",
    "        process_item=lambda seq, tokenizer: process_item(\n",
    "            seq, \n",
    "            context_length=encodon_model.model.hparams.max_position_embeddings,\n",
    "            tokenizer=tokenizer\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    print(f\"Processing {len(dataset)} sequences\")\n",
    "    print(f\"Target range: [{dataset.data['value'].min():.3f}, {dataset.data['value'].max():.3f}]\")\n",
    "    \n",
    "    # Create data loader for batch processing\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Extract embeddings using the dataset\n",
    "    print(\"\\nExtracting embeddings...\")\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch in tqdm(dataloader):\n",
    "        batch_input = {\n",
    "            MetadataFields.INPUT_IDS: batch[MetadataFields.INPUT_IDS].to(encodon_model.device),\n",
    "            MetadataFields.ATTENTION_MASK: batch[MetadataFields.ATTENTION_MASK].to(encodon_model.device),\n",
    "        }\n",
    "        \n",
    "        # Extract embeddings\n",
    "        output = encodon_model.extract_embeddings(batch_input)\n",
    "        all_embeddings.append(output.embeddings)\n",
    "        all_labels.append(batch[MetadataFields.LABELS].numpy())\n",
    "    \n",
    "    # Combine all embeddings and labels\n",
    "    embeddings = np.vstack(all_embeddings)\n",
    "    targets = np.concatenate(all_labels)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Extracted embeddings: {embeddings.shape}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Skipping preprocessing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 5. Train Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'embeddings' in locals():\n",
    "    print(\"=== TRAINING RANDOM FOREST ===\")\n",
    "    \n",
    "    # Split data based on the dataset splits\n",
    "    train_mask = dataset.data['split'] == 'train'\n",
    "    val_mask = dataset.data['split'] == 'val'\n",
    "    test_mask = dataset.data['split'] == 'test'\n",
    "    \n",
    "    X_train = embeddings[train_mask]\n",
    "    X_val = embeddings[val_mask]\n",
    "    X_test = embeddings[test_mask]\n",
    "    y_train = targets[train_mask]\n",
    "    y_val = targets[val_mask]\n",
    "    y_test = targets[test_mask]\n",
    "    \n",
    "    print(f\"Train: {X_train.shape[0]}, Val: {X_val.shape[0]}, Test: {X_test.shape[0]}\")\n",
    "    \n",
    "    # Combine train and validation for GridSearchCV\n",
    "    X_train_val = np.vstack([X_train, X_val])\n",
    "    y_train_val = np.concatenate([y_train, y_val])\n",
    "    \n",
    "    # Create validation indices for GridSearchCV\n",
    "    # Train indices: 0 to len(X_train)-1\n",
    "    # Val indices: len(X_train) to len(X_train_val)-1\n",
    "    train_indices = list(range(len(X_train)))\n",
    "    val_indices = list(range(len(X_train), len(X_train_val)))\n",
    "    cv_splits = [(train_indices, val_indices)]\n",
    "    \n",
    "    # Define hyperparameter grid\n",
    "    param_grid = {\n",
    "        'n_estimators': [1000],\n",
    "        'max_depth': [10],\n",
    "        'min_samples_split': [25],\n",
    "        'min_samples_leaf': [2],\n",
    "    }\n",
    "    \n",
    "    # Create base model\n",
    "    rf_base = RandomForestRegressor(random_state=42, n_jobs=-1)\n",
    "    \n",
    "    # Grid search with validation split\n",
    "    print(\"Performing hyperparameter tuning...\")\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=rf_base,\n",
    "        param_grid=param_grid,\n",
    "        cv=cv_splits,\n",
    "        scoring='r2',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit grid search\n",
    "    grid_search.fit(X_train_val, y_train_val)\n",
    "    \n",
    "    # Get best model\n",
    "    rf = grid_search.best_estimator_\n",
    "    \n",
    "    print(f\"\\n=== BEST PARAMETERS ===\")\n",
    "    for param, value in grid_search.best_params_.items():\n",
    "        print(f\"{param}: {value}\")\n",
    "    print(f\"Best validation R¬≤: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Train final model on train set only\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions on all splits\n",
    "    y_pred_train = rf.predict(X_train)\n",
    "    y_pred_val = rf.predict(X_val)\n",
    "    y_pred_test = rf.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics for all splits\n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    val_r2 = r2_score(y_val, y_pred_val)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    \n",
    "    train_spearmanr, _ = spearmanr(y_train, y_pred_train)\n",
    "    val_spearmanr, _ = spearmanr(y_val, y_pred_val)\n",
    "    test_spearmanr, _ = spearmanr(y_test, y_pred_test)\n",
    "    \n",
    "    print(f\"\\n=== FINAL RESULTS ===\")\n",
    "    print(f\"Train R¬≤: {train_r2:.4f} | spearmanr r: {train_spearmanr:.4f}\")\n",
    "    print(f\"Val R¬≤:   {val_r2:.4f} | spearmanr r: {val_spearmanr:.4f}\")\n",
    "    print(f\"Test R¬≤:  {test_r2:.4f} | spearmanr r: {test_spearmanr:.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Cannot train - missing data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 6. Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'y_test' in locals():\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig.suptitle('mRNA Stability Prediction Results', fontsize=16)\n",
    "    \n",
    "    # Predicted vs True for all splits\n",
    "    splits = [('Train', y_train, y_pred_train, train_r2), \n",
    "              ('Validation', y_val, y_pred_val, val_r2), \n",
    "              ('Test', y_test, y_pred_test, test_r2)]\n",
    "    \n",
    "    for i, (split_name, y_true, y_pred, r2) in enumerate(splits):\n",
    "        axes[0, i].scatter(y_true, y_pred, alpha=0.6)\n",
    "        min_val = min(y_true.min(), y_pred.min())\n",
    "        max_val = max(y_true.max(), y_pred.max())\n",
    "        axes[0, i].plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2)\n",
    "        axes[0, i].set_xlabel('True mRNA Stability')\n",
    "        axes[0, i].set_ylabel('Predicted mRNA Stability')\n",
    "        axes[0, i].set_title(f'{split_name}\\nR¬≤ = {r2:.3f}')\n",
    "        axes[0, i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Performance comparison\n",
    "    r2_scores = [train_r2, val_r2, test_r2]\n",
    "    spearmanr_scores = [train_spearmanr, val_spearmanr, test_spearmanr]\n",
    "    \n",
    "    x_pos = np.arange(len(splits))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[1, 0].bar(x_pos - width/2, r2_scores, width, label='R¬≤', alpha=0.7)\n",
    "    axes[1, 0].bar(x_pos + width/2, spearmanr_scores, width, label='spearmanr r', alpha=0.7)\n",
    "    axes[1, 0].set_xlabel('Dataset Split')\n",
    "    axes[1, 0].set_ylabel('Score')\n",
    "    axes[1, 0].set_title('Performance Comparison')\n",
    "    axes[1, 0].set_xticks(x_pos)\n",
    "    axes[1, 0].set_xticklabels(['Train', 'Val', 'Test'])\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Target distribution across splits\n",
    "    axes[1, 1].hist([y_train, y_val, y_test], bins=15, alpha=0.7, \n",
    "                   label=['Train', 'Val', 'Test'], edgecolor='black')\n",
    "    axes[1, 1].set_xlabel('mRNA Stability')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].set_title('Target Distribution by Split')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Feature importance\n",
    "    top_features = np.argsort(rf.feature_importances_)[-10:]\n",
    "    axes[1, 2].barh(range(10), rf.feature_importances_[top_features])\n",
    "    axes[1, 2].set_xlabel('Importance')\n",
    "    axes[1, 2].set_title('Top 10 Feature Importances')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå No results to plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## 7. Troubleshooting & Optimization Tips\n",
    "\n",
    "### Common Issues and Solutions:\n",
    "\n",
    "#### 1. Model Loading Issues\n",
    "- **Problem**: Checkpoint not found\n",
    "- **Solution**: Update checkpoint paths in section 2\n",
    "- **Check**: Verify checkpoint files exist and are accessible\n",
    "\n",
    "#### 2. Data Loading Issues\n",
    "- **Problem**: Dataset not found\n",
    "- **Solution**: Update data paths in section 3\n",
    "- **Check**: Ensure CSV files have required columns (id, ref_seq, value)\n",
    "\n",
    "#### 3. Memory Issues\n",
    "- **Problem**: CUDA out of memory\n",
    "- **Solution**: Reduce batch_size in preprocessing section\n",
    "- **Alternative**: Use CPU by setting device='cpu'\n",
    "\n",
    "#### 4. Performance Issues\n",
    "- **Problem**: Low R¬≤ scores\n",
    "- **Solutions**:\n",
    "  - Try larger models (600M or 1B parameters)\n",
    "  - Implement fine-tuning instead of just embeddings\n",
    "  - Tune Random Forest hyperparameters\n",
    "  - Check data quality and preprocessing\n",
    "\n",
    "\n",
    "### Optimization Strategies:\n",
    "\n",
    "#### 1. Model Architecture\n",
    "- **80M model**: Fast, good for initial experiments\n",
    "- **600M model**: Better performance, moderate cost\n",
    "- **1B model**: Best performance, highest computational cost\n",
    "\n",
    "#### 3. Hyperparameter Tuning\n",
    "```python\n",
    "# Try these Random Forest parameters:\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 200, 500],\n",
    "    'max_depth': [10, 15, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
