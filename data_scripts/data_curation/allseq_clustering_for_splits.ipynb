{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fd126bb",
   "metadata": {},
   "source": [
    "## Convert sequences to amino acids\n",
    "\n",
    "Create Fasta files of amino acid sequences for alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3173e19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- User-configurable paths ---\n",
    "# Set these to match your environment.\n",
    "# The `NCBI_GROUPED_DIR` is produced by `data_scripts/data_curation/download_cds_clean.ipynb`.\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path('/workspace')  # Path to the repo root\n",
    "NCBI_GROUPED_DIR = Path('/data/ncbi_grouped')   # Grouped CSVs per organism group\n",
    "AA_OUTPUT_DIR = Path('/data/ncbi_grouped_aa')   # Output folder for generated FASTA files\n",
    "\n",
    "# MMseqs2 working directory (contains allSeqClust.* and allSeqs.lookup)\n",
    "MMSEQS_WORK_DIR = Path('/data/codonfm_mmseqs/ncbi')\n",
    "\n",
    "# Dataset storage for mmap and cache\n",
    "DATASET_DIR = Path('/workspace/codonfm_mmseqs/temp_save')\n",
    "CACHE_PATH = Path('/data/codonfm_mmseqs/ncbi/global_index.cache.npy')\n",
    "\n",
    "# Where to save final clustering outputs\n",
    "CLUSTERS_OUTPUT_DIR = Path('clusters')\n",
    "CLUSTERS_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Ensure output directories exist\n",
    "AA_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5857193",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import polars as pl\n",
    "from Bio.Seq import Seq\n",
    "\n",
    "# Input grouped CSVs and output FASTA directory\n",
    "files = sorted(glob(str(NCBI_GROUPED_DIR / '*.csv')))\n",
    "\n",
    "def translate_cds(cds_seq):\n",
    "    try:\n",
    "        return str(Seq(cds_seq).translate(to_stop=False))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "for fn in files:\n",
    "    data = pl.read_csv(fn)\n",
    "\n",
    "    group = os.path.basename(fn).split('.')[0]\n",
    "\n",
    "    data = data.with_columns(\n",
    "        pl.col('cds').map_elements(translate_cds, return_dtype=pl.Utf8).alias('amino_acid')\n",
    "    )\n",
    "    out_fa = AA_OUTPUT_DIR / f'{group}.fa'\n",
    "    with open(out_fa, 'w') as f:\n",
    "        for i, row in enumerate(data.iter_rows(named=True)):\n",
    "            f.write(f'>{group}_{i}\\n{row[\"amino_acid\"]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca911ce7",
   "metadata": {},
   "source": [
    "## Use MMSeqs to cluster the sequences\n",
    "\n",
    "MMseqs2 is required for the clustering step. If you don't have it installed, install it first:\n",
    "\n",
    "- Recommended (Conda/):\n",
    "```bash\n",
    "conda install -c conda-forge -c bioconda mmseqs2\n",
    "```\n",
    "- Ubuntu (apt, may be older version):\n",
    "```bash\n",
    "sudo apt-get update && sudo apt-get install -y mmseqs2\n",
    "```\n",
    "- Docker (no local install needed):\n",
    "```bash\n",
    "docker run --rm -it -v \"$PWD\":\"/work\" soedinglab/mmseqs2:latest bash\n",
    "# Then run the commands below inside the container in /work\n",
    "```\n",
    "\n",
    "Once installed, run the following from the directory containing your `.fa` files (each file holds amino-acid sequences):\n",
    "\n",
    "```bash\n",
    "# Create a temporary working directory for MMseqs2\n",
    "mkdir -p alltmp\n",
    "\n",
    "# Create an MMseqs2 database from all fasta files in the folder\n",
    "mmseqs createdb *.fa allSeqs\n",
    "\n",
    "# Cluster sequences at 50% identity, 90% coverage (cov-mode 5)\n",
    "mmseqs linclust allSeqs allSeqClust alltmp \\\n",
    "  --min-seq-id 0.5 -c 0.9 --cov-mode 5 \\\n",
    "  --threads \"$(nproc)\"\n",
    "```\n",
    "\n",
    "Notes:\n",
    "- This notebook expects the outputs `allSeqClust.*` and `allSeqs.lookup` to be generated in the working directory.\n",
    "- Increase or decrease `--min-seq-id`/`-c` as needed for your clustering granularity.\n",
    "- If `mmseqs` is not found, ensure it is on your PATH (e.g., `conda activate <env>`).\n",
    "\n",
    "``` \n",
    "mmseqs createdb *.fa allSeqs\n",
    "mmseqs linclust allSeqs allSeqClust alltmp --min-seq-id 0.5 -c 0.9 --cov-mode 5\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3826f8",
   "metadata": {},
   "source": [
    "**Once the above are generated, you can run the following to map the sequences to clusters:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786c4491",
   "metadata": {},
   "source": [
    "## Map the sequences to clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2512bc7c-a2da-41be-b212-4f787c6d6294",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import polars as pl\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('/workspace/codon-fm') # NOTE: this assumes you've launched the notebook from /workspace and the codon-fm repo is mounted at /workspace/codon-fm\n",
    "from src.tokenizer import Tokenizer\n",
    "import torch\n",
    "from typing import Callable, List\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828d4618-0549-4433-bf4e-ac26bbe8b30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class CodonMmapDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,\n",
    "                 data_path: str,\n",
    "                 cache_path: str,\n",
    "                 tokenizer: Callable,\n",
    "                 seed: int = 42):\n",
    "\n",
    "        self.data_path = Path(data_path)\n",
    "        self.metadata_path = self.data_path / \"metadata.json\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.cache_path = Path(cache_path)\n",
    "\n",
    "        with open(self.metadata_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "            \n",
    "        self.chunks_metadata = metadata['chunks']\n",
    "\n",
    "        self.group_offset = {}\n",
    "        curr = 0\n",
    "        for item in metadata['file_metadata']:\n",
    "            gr = item['file_name'].split('.csv')[0]\n",
    "            if gr not in self.group_offset:\n",
    "                self.group_offset[gr] = curr\n",
    "            curr += item['end']-item['start']+1\n",
    "\n",
    "\n",
    "        if self.cache_path.exists():\n",
    "            print(\"Loading cached global indices...\")\n",
    "            self.global_indices = np.load(cache_path, allow_pickle=True)#.tolist()\n",
    "        else:\n",
    "            self.indices_mmaps = []\n",
    "            for chunk in self.chunks_metadata:\n",
    "                \n",
    "                idx_mmap_path = self.data_path / chunk['index']['path']\n",
    "    \n",
    "                idx_mmap = np.memmap(idx_mmap_path,\n",
    "                                     dtype=chunk['index']['dtype'],\n",
    "                                     mode='r',\n",
    "                                     shape=tuple(chunk['index']['shape']))\n",
    "    \n",
    "                self.indices_mmaps.append(idx_mmap)\n",
    "            print(\"Computing global indices for subsequences...\")\n",
    "            self.global_indices = []\n",
    "            for chunk_id, idx_mmap in enumerate(self.indices_mmaps):\n",
    "                for seq_idx in tqdm(range(len(idx_mmap))):\n",
    "                    seq_start, seq_end, taxid = idx_mmap[seq_idx]\n",
    "\n",
    "                    self.global_indices.append((chunk_id, seq_start, seq_end))\n",
    "\n",
    "            np.save(self.cache_path, np.array(self.global_indices, dtype=np.uint32))\n",
    "            print(f\"Cached global indices saved at {cache_path}\")\n",
    "\n",
    "    def get_by_group(self, group, group_idx):\n",
    "        idx = self.group_offset[group] + group_idx\n",
    "        # return idx\n",
    "        return self.__getitem__(idx)\n",
    "\n",
    "    def idx_by_group(self, group, group_idx):\n",
    "        idx = self.group_offset[group] + group_idx\n",
    "        return idx\n",
    "        # return self.__getitem__(idx)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.global_indices)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk_id, start_token_idx, end_token_idx = self.global_indices[idx]\n",
    "        chunk = self.chunks_metadata[chunk_id]\n",
    "        seq_mmap_path = self.data_path / chunk['sequences']['path']\n",
    "        seq_mmap = np.memmap(seq_mmap_path,\n",
    "                                 dtype=chunk['sequences']['dtype'],\n",
    "                                 mode='r',\n",
    "                                 shape=tuple(chunk['sequences']['shape']))\n",
    "        sequence_tokens = seq_mmap[start_token_idx:end_token_idx]\n",
    "        return sequence_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1d9453-fabf-4bee-bdd3-3411b86ed3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached global indices...\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(\n",
    "            cls_token=\"<CLS>\",\n",
    "            bos_token=\"<CLS>\",\n",
    "            sep_token=\"<SEP>\",\n",
    "            unk_token=\"<UNK>\",\n",
    "            pad_token=\"<PAD>\",\n",
    "            mask_token=\"<MASK>\",\n",
    "            padding_side=\"right\",\n",
    "            truncation=\"right\",\n",
    "            seq_type=\"dna\",\n",
    "        )\n",
    "dataset = CodonMmapDataset(DATASET_DIR, \n",
    "                           cache_path=CACHE_PATH,\n",
    "                          tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fb796b-880e-4082-80c5-18b5e9feb406",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:49<00:00,  1.94it/s]\n"
     ]
    }
   ],
   "source": [
    "clusters = []\n",
    "for fn in tqdm(sorted(glob(str(MMSEQS_WORK_DIR / 'allSeqClust.[0-9]*')))):\n",
    "    with open(fn, 'rb') as f:\n",
    "        temp = f.read().split(b'\\x00')\n",
    "        temp = [list(map(int, x.strip().split())) for x in temp]\n",
    "        clusters += temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbff2e6f-66af-4b0f-99c9-7535951a2a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup = pl.read_csv(str(MMSEQS_WORK_DIR / 'allSeqs.lookup'), separator='\\t', has_header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac7ce087-6e52-45cd-9b82-63aacb48f9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "lookup.columns = ['seq_idx','seq_name','index']\n",
    "lookup = lookup.select(['seq_idx','seq_name']).with_row_index()\n",
    "lookup_seqs = lookup['seq_name'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "964ee914-2db8-46de-8bc4-f2d942c3f8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_g_i(gi):\n",
    "    parts = gi.rsplit('_', 1)  # Split only once, from the right\n",
    "    g = parts[0]\n",
    "    i = int(parts[1])\n",
    "    return g, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cdfc5a06-4f4a-4972-88bf-be3159cd10c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41422336/41422336 [03:22<00:00, 204460.04it/s]\n"
     ]
    }
   ],
   "source": [
    "seq_name_clusters = []\n",
    "global_group_idx = [-1] * len(dataset)\n",
    "\n",
    "for cluster_i,cluster in enumerate(tqdm(clusters)):\n",
    "    if cluster:\n",
    "        # curr_gi = [get_g_i(lookup_seqs[i]) for i in cluster]\n",
    "        curr_gi = [lookup_seqs[i] for i in cluster]\n",
    "        seq_name_clusters.append(curr_gi)\n",
    "    \n",
    "        for g,i in map(get_g_i, curr_gi):\n",
    "            idx = dataset.idx_by_group(g,i)\n",
    "            global_group_idx[idx] = cluster_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e000a01-98fc-4326-888c-5b83357780ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(CLUSTERS_OUTPUT_DIR / 'allSeqClusterIdx.npy', np.array(global_group_idx))\n",
    "\n",
    "# NOTE: this file should be moved to the directory that contains the memmap files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
