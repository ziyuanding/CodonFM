# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from typing import Any, Dict
import os
from pathlib import Path

import torch
import lightning.pytorch as L
import fiddle as fdl
from lightning.pytorch import seed_everything, Trainer
from src.utils import RankedLogger
from safetensors.torch import load_file

# Initialize logger at module level so it's available to all functions
logging = RankedLogger(__name__, rank_zero_only=True)


def train(config: Dict[str, Any], 
          ckpt_path: str, 
          seed: int, 
          config_dict: Dict[str, Any], 
          out_dir: str):
    """Launches the pre-training process for the Encodon model.

    Args:
        config: A dictionary containing the configuration for the model, data, and trainer.
        ckpt_path: The path to the checkpoint file to resume training from.
        seed: The random seed to use for reproducibility.
    """
    seed_everything(seed, workers=True)
    
    if not os.path.exists(out_dir):
        os.makedirs(out_dir)
    
    logger, data, trainer_kwargs, model, callbacks = config["log"], \
                                              config["data"], \
                                              config["trainer"], \
                                              config["model"], \
                                              config["callbacks"]
    trainer = Trainer(**trainer_kwargs)
    
    if logger and hasattr(logger, "log_hyperparams"):
        logger.log_hyperparams(config_dict)
        
    if os.path.exists(ckpt_path):
        state_dict = torch.load(ckpt_path, map_location="cpu")
        model.configure_model(state_dict=state_dict.get("state_dict"))
    else:
        model.configure_model()
    
    trainer.callbacks = list(callbacks.values())
    trainer.logger = logger
    logging.info(f"Starting pre-training from {ckpt_path}")
    trainer.fit(model, datamodule=data, ckpt_path=ckpt_path if os.path.exists(ckpt_path) else None)

def finetune(config: Dict[str, Any],
             pretrained_ckpt_path: str,
             seed: int,
             resume_trainer_state: bool,
             config_dict: Dict[str, Any], 
             out_dir: str,
             ckpt_path: str):
    """Launches the fine-tuning process for the Encodon model.

    Args:
        config: A dictionary containing the configuration for the model, data, and trainer.
        ckpt_path: The path to save the fine-tuned model checkpoint.
        pretrained_ckpt_path: The path to the pre-trained model checkpoint to start fine-tuning from.
        seed: The random seed to use for reproducibility.
        resume_trainer_state: Whether to resume the trainer state from the checkpoint.
    """
    seed_everything(seed, workers=True)
    
    if not os.path.exists(out_dir):
        os.makedirs(out_dir)
    
    logger, data, trainer_kwargs, model, callbacks = config["log"], \
                                              config["data"], \
                                              config["trainer"], \
                                              config["model"], \
                                              config["callbacks"]
    trainer = Trainer(**trainer_kwargs)
    
    if logger and hasattr(logger, "log_hyperparams"):
        logger.log_hyperparams(config_dict)
        
    if os.path.exists(pretrained_ckpt_path) and not os.path.exists(ckpt_path):
        # first time finetuning from a pretrained checkpoint: can be a .safetensors or a .ckpt file.
        ckpt_suffix = Path(pretrained_ckpt_path).suffix.lower()
        if ckpt_suffix == ".safetensors":
            state_dict = load_file(str(Path(pretrained_ckpt_path)))
            model.configure_model(state_dict=state_dict)
            trainer_ckpt_path = None
        else:
            state_dict = torch.load(pretrained_ckpt_path, map_location="cpu")
            model.configure_model(state_dict=state_dict.get("state_dict"))
    else:
        logging.info(f"No pretrained checkpoint found at {pretrained_ckpt_path}, starting from scratch")
        model.configure_model()
    
    trainer.callbacks = list(callbacks.values())
    trainer.logger = logger
    
    if resume_trainer_state and os.path.exists(pretrained_ckpt_path) and not os.path.exists(ckpt_path):
        # Assert a *.ckpt file is provided when resuming trainer state.
        assert Path(pretrained_ckpt_path).suffix.lower() == ".ckpt", "Pretrained checkpoint must be a *.ckpt file when resuming trainer state."
        trainer_ckpt_path = pretrained_ckpt_path
    else:
        trainer_ckpt_path = ckpt_path if os.path.exists(ckpt_path) else None
    
    logging.info(f"Starting finetuning from {trainer_ckpt_path} \
        with resume_trainer_state={resume_trainer_state} and ckpt_path={ckpt_path}")
    trainer.fit(model, datamodule=data, ckpt_path=trainer_ckpt_path)


def evaluate(
    config: Dict[str, Any],
    config_dict: Dict[str, Any],
    model_ckpt_path: str,
    out_dir: str,
    seed: int = 123,
) -> None:
    """Launches the evaluation process for the Encodon model.

    Args:
        config: A dictionary containing the configuration for the model, data, and trainer.
        config_dict: A dictionary containing the configuration for the model, data, and trainer.
        
    Note: Evaluation must be run in a single run as resuming the trainer state is not supported for prediction.
    """
    seed_everything(seed, workers=True)
    if not os.path.exists(out_dir):
        os.makedirs(out_dir)

    logger, data, trainer_kwargs, model, callbacks = config["log"], \
                                          config["data"], \
                                          config["trainer"], \
                                          config["model"], \
                                          config["callbacks"]
    
    trainer = Trainer(**trainer_kwargs)
    
    if logger and hasattr(logger, "log_hyperparams"):
        logger.log_hyperparams(config_dict)
        
    model.configure_model()

    data.setup("test")
    if os.path.exists(model_ckpt_path):
        logging.info(f"Loading dataset checkpoint from {model_ckpt_path}")
        data.load_state_dict(torch.load(model_ckpt_path))
        model.prediction_counter = data.init_global_step
    
    trainer.logger = logger
    trainer.callbacks = list(callbacks.values())
    
    logging.info("Starting Evaluation!")
    trainer.predict(model, 
                    datamodule=data, 
                    return_predictions=False)
    return 