# SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# SPDX-License-Identifier: Apache-2.0
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from typing import Dict

import torch
import numpy as np
import torch.distributed as dist
from safetensors.torch import load_file
import json
from pathlib import Path

from src.tokenizer import Tokenizer
from src.inference.base import BaseInference
from src.inference.task_types import TaskTypes
from src.inference.model_outputs import (
    MaskedLMOutput,
    MutationPredictionOutput,
    FitnessPredictionOutput,
    EmbeddingOutput,
    DownstreamPredictionOutput
)
from src.models.encodon_pl import EncodonPL
from src.data.metadata import MetadataFields


class EncodonInference(BaseInference):
    """Inference class for Encodon models."""
    def configure_model(self):
        """Loads the model and tokenizer for inference."""
        if self.model is not None:
            return
        self.tokenizer = Tokenizer()
        
        state_dict = None
        hparams = None
        
        # Expect a full file path to either a .ckpt or a .safetensors file
        model_path = Path(self.model_path)
        if model_path.suffix.lower() not in [".ckpt", ".safetensors"]:
            raise ValueError(
                f"Expected a file path to a .ckpt or .safetensors file, got: {self.model_path}"
            )
        if not model_path.is_file():
            raise FileNotFoundError(f"Model file not found at {self.model_path}")

        suffix = model_path.suffix.lower()
        if suffix == ".safetensors":
            # Ensure config.json exists in the same directory
            config_path = model_path.parent / "config.json"
            if not config_path.exists():
                raise FileNotFoundError(
                    f"config.json is required to load safetensors checkpoint at: {config_path}"
                )
            if dist.is_initialized():
                broadcasted_objects = [None, None]
                if dist.get_rank() == 0:
                    state_dict = load_file(str(model_path))
                    with open(config_path, 'r') as f:
                        hparams = json.load(f)
                    broadcasted_objects = [state_dict, hparams]
                dist.broadcast_object_list(broadcasted_objects, src=0)
                state_dict, hparams = broadcasted_objects
            else:
                state_dict = load_file(str(model_path))
                with open(config_path, 'r') as f:
                    hparams = json.load(f)
        elif suffix == ".ckpt":
            # Load from PyTorch Lightning checkpoint
            if dist.is_initialized():
                broadcasted_objects = [None, None]
                if dist.get_rank() == 0:
                    ckpt = torch.load(self.model_path, map_location="cpu")
                    hparams = ckpt.get("hyper_parameters")
                    state_dict = ckpt.get("state_dict")
                    broadcasted_objects = [state_dict, hparams]
                dist.broadcast_object_list(broadcasted_objects, src=0)
                state_dict, hparams = broadcasted_objects
            else:
                ckpt = torch.load(self.model_path, map_location="cpu")
                hparams = ckpt.get("hyper_parameters")
                state_dict = ckpt.get("state_dict")
        else:
            raise ValueError(
                f"Unsupported model file type: {suffix}. Expected .ckpt or .safetensors"
            )

        # The hparams from lightning checkpoint might be nested.
        if 'hparams' in hparams:
            hparams = hparams['hparams']
        
        # For inference we don't need optimizer and scheduler, but EncodonPL expects them.
        def dummy_optimizer(params):
            return torch.optim.Adam(params)
        
        hparams['optimizer'] = dummy_optimizer
        hparams['scheduler'] = None

        self.model = EncodonPL(**hparams)
        self.model.configure_model(state_dict=state_dict)
        self.model.to(self.device)
        self.model.eval()
        
    def predict_mlm(self, batch, ids=None) -> Dict[str, np.ndarray]:
        """Predicts masked tokens in a batch."""
        with torch.no_grad():
            output = self.model(
                batch
            )
            preds = output.logits
            if preds.dtype != torch.float:
                preds = preds.float()
            mask = batch[MetadataFields.INPUT_MASK].bool()
            y = batch[MetadataFields.LABELS]
            y = y[mask]
            preds = preds[mask]
            preds = preds.cpu().numpy()
            if len(y) > 0:
                y = y.cpu().numpy()
            
        return MaskedLMOutput(preds=preds, labels=y, ids=ids)
        
    def predict_mutation(self, batch, ids=None) -> Dict[str, np.ndarray]:
        """
        Predicts the effect of a mutation by calculating the log-likelihood ratio 
        of the reference codon vs. the alternative codon at the mutation site.
        """
        with torch.no_grad():
            output = self.model(
                batch
            )
            preds = output.logits
            if preds.dtype != torch.float:
                preds = preds.float()
            ref_toks = batch[MetadataFields.REF_CODON_TOKS]
            alt_toks = batch[MetadataFields.ALT_CODON_TOKS]
            mutation_token_idx = batch[MetadataFields.MUTATION_TOKEN_IDX].view(-1)
            # Get predictions only for the mutated token positions.
            preds = preds[torch.arange(preds.shape[0]), mutation_token_idx, :]
            # Convert logits to log-probabilities.
            preds = torch.nn.functional.log_softmax(preds, dim=-1)
            # Get the log-likelihoods for the reference and alternate codons.
            ref_likelihoods = preds[torch.arange(preds.shape[0]), ref_toks]
            alt_likelihoods = preds[torch.arange(preds.shape[0]), alt_toks]
            # The likelihood ratio is the difference in log-likelihoods.
            likelihood_ratios = ref_likelihoods - alt_likelihoods
        return MutationPredictionOutput(
            ref_likelihoods=ref_likelihoods.cpu().numpy(),
            alt_likelihoods=alt_likelihoods.cpu().numpy(),
            likelihood_ratios=likelihood_ratios.cpu().numpy(),
            ids=ids,
        )
        
    def extract_embeddings(self, batch, ids=None) -> Dict[str, np.ndarray]:
        """Extracts embeddings for a batch of sequences."""
        with torch.no_grad():
            output = self.model(
                batch,
                return_hidden_states=True
            )
            embeddings = output.all_hidden_states[-1]
            if embeddings.dtype != torch.float:
                embeddings = embeddings.float()
            embeddings = embeddings[:, 0, :] # [CLS] token
            embeddings = embeddings.cpu().numpy()
        return EmbeddingOutput(embeddings=embeddings, ids=ids)
    
    def predict_fitness(self, batch, ids=None) -> Dict[str, np.ndarray]:
        """
        Predicts a fitness score for each sequence in the batch.
        The fitness score is defined as the average log-likelihood of the sequence.
        """
        with torch.no_grad():
            output = self.model(
                batch
            )
            preds = output.logits
            if preds.dtype != torch.float:
                preds = preds.float()
            
            # Get log-probabilities for all tokens in the vocabulary.
            log_probs = torch.nn.functional.log_softmax(preds, dim=-1)  
            # Gather the log-probabilities of the input tokens.
            selected_log_probs = log_probs.gather(-1, batch[MetadataFields.INPUT_IDS].unsqueeze(-1)).squeeze(-1)
            # Create a mask to exclude padding tokens from the calculation.
            non_padding_mask = batch[MetadataFields.INPUT_IDS] != self.tokenizer.pad_token_id  
            # Apply the mask to zero out log-probabilities of padding tokens.
            masked_log_probs = selected_log_probs * non_padding_mask
            # Sum the log-likelihoods for each sequence, ignoring padding.
            log_likelihoods_sum = masked_log_probs.sum(dim=-1)
            # Count the number of non-padding tokens in each sequence.
            non_padding_counts = non_padding_mask.sum(dim=-1)
            # Compute the mean log-likelihood per sequence.
            log_likelihoods_mean = (log_likelihoods_sum / non_padding_counts).cpu().numpy()
        return FitnessPredictionOutput(fitness=log_likelihoods_mean, ids=ids)

    def predict_downstream(self, batch, ids=None) -> DownstreamPredictionOutput:
        """
        Predicts using the downstream cross-attention head (classification or regression).
        This works with models that have use_downstream_head=True.
        """
        with torch.no_grad():
            # Check if model has downstream heads
            if not hasattr(self.model.model, 'cross_attention_head') or not hasattr(self.model.model, 'cross_attention_input_proj'):
                raise ValueError("Model does not have downstream cross-attention heads. Ensure the model was trained with use_downstream_head=True.")
            
            # Get the base model output (hidden states)
            output = self.model(batch)
            hidden_states = output.last_hidden_state  # [batch_size, seq_len, hidden_size]
            attention_mask = batch[MetadataFields.ATTENTION_MASK]  # [batch_size, seq_len]
            
            # Project hidden states to cross-attention dimension
            projected_states = self.model.model.cross_attention_input_proj(hidden_states)  # [batch_size, seq_len, cross_attn_hidden_dim]
            
            # Extract [CLS] token as query and use full sequence as key/value
            query_input = projected_states[:, 0, :]  # [batch_size, cross_attn_hidden_dim]
            key_value_input = projected_states  # [batch_size, seq_len, cross_attn_hidden_dim]
            
            # Pass through cross-attention head
            preds = self.model.model.cross_attention_head(query_input, key_value_input, attention_mask)
            
            # Determine task type based on model configuration
            loss_type = getattr(self.model.hparams, 'loss_type', 'regression')
            
            if loss_type == "classification":
                # Classification: preds shape [batch_size, num_classes]
                preds = preds.float().cpu().numpy()
                
                # Get probabilities (softmax) and predicted classes
                probabilities = torch.nn.functional.softmax(torch.from_numpy(preds), dim=-1).numpy()
                predicted_classes = np.argmax(preds, axis=-1)
                
                return DownstreamPredictionOutput(
                    predictions=preds,
                    probabilities=probabilities,
                    predicted_classes=predicted_classes,
                    ids=ids
                )
            else:
                # Regression: preds shape [batch_size, 1] -> squeeze to [batch_size]
                preds = preds.squeeze(-1).float().cpu().numpy()
                
                return DownstreamPredictionOutput(
                    predictions=preds,
                    ids=ids
                )

    def _predict_step(self, batch, batch_idx):
        """A single prediction step that dispatches to the correct prediction function
        based on the task type.
        """
        # - remove id column from batch if present
        #  (this is needed for the model forward to work correctly)
        ids = None
        if MetadataFields.ID in batch:
            ids = batch[MetadataFields.ID]
            del batch[MetadataFields.ID]
        # - set predict function based on task type
        if self.task_type == TaskTypes.MUTATION_PREDICTION:
            predict = self.predict_mutation
        elif self.task_type == TaskTypes.MASKED_LANGUAGE_MODELING:
            predict = self.predict_mlm
        elif self.task_type == TaskTypes.FITNESS_PREDICTION:
            predict = self.predict_fitness
        elif self.task_type == TaskTypes.EMBEDDING_PREDICTION:
            predict = self.extract_embeddings
        elif self.task_type == TaskTypes.DOWNSTREAM_PREDICTION:
            predict = self.predict_downstream
        else:
            raise ValueError(f"Unsupported task type: {self.task_type}")
        
        outputs = predict(batch, ids)
        return outputs